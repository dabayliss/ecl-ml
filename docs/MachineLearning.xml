<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.1.2//EN"
"http://www.oasis-open.org/docbook/xml/4.1.2/docbookx.dtd">
<book lang="en_US">
  <bookinfo>
    <title>Machine Learning Library Reference</title>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/redswooshWithLogo3.jpg" />
      </imageobject>
    </mediaobject>

    <author>
      <surname>Boca Raton Documentation Team</surname>
    </author>

    <legalnotice>
      <para>We welcome your comments and feedback about this document via
      email to <email>docfeedback@hpccsystems.com</email> Please include
      <emphasis role="bold">Documentation Feedback</emphasis> in the subject
      line and reference the document name, page numbers, and current Version
      Number in the text of the message.</para>

      <para>LexisNexis and the Knowledge Burst logo are registered trademarks
      of Reed Elsevier Properties Inc., used under license. Other products,
      logos, and services may be trademarks or registered trademarks of their
      respective companies. All names and example data used in this manual are
      fictitious. Any similarity to actual persons, living or dead, is purely
      coincidental.</para>

      <para></para>
    </legalnotice>

    <releaseinfo>© 2013 HPCC Systems. All rights reserved</releaseinfo>

    <date>July 2013 Version 1.2.0C</date>

    <corpname>HPCC Systems</corpname>

    <copyright>
      <year>2013 HPCC Systems. All rights reserved</year>
    </copyright>

    <mediaobject role="logo">
      <imageobject>
        <imagedata fileref="images/iStock_000012344803XSmall.jpg" />
      </imageobject>
    </mediaobject>
  </bookinfo>

  <chapter id="Introduction_Installation">
    <title>Introduction and Installation</title>

    <para>LexisNexis Risk Solutions is an industry leader in data content,
    data aggregation, and information services which has independently
    developed and implemented a solution for data-intensive computing called
    HPCC (High-Performance Computing Cluster).</para>

    <para>The HPCC System is designed to run on clusters, leveraging the
    resources across all nodes. However, it can also be installed on a single
    machine and/or VM for learning or test purposes. It will also run on the
    AWS Cloud.</para>

    <para>The HPCC platform also includes ECL (Enterprise Control Language)
    which is a powerful high-level, heavily-optimized, data-centric
    declarative language used for parallel data processing. The flexibility of
    the ECL language is such that any ECL code will run unmodified regardless
    of the size of the cluster being used.</para>

    <para>Instructions for installing the HPCC are available on the HPCC
    Systems website, <ulink
    url="http://hpccsystems.com/community/docs/installation-and-administration">http://hpccsystems.com/community/docs/installation-and-administration</ulink>.
    More information about running HPCC on the AWS Cloud can be found in this
    document, <emphasis>Running the HPCC System's Thor Platform within Amazon
    Web Services</emphasis>, <ulink
    url="http://hpccsystems.com/community/docs/aws-install-thor">http://hpccsystems.com/community/docs/aws-install-thor</ulink>.</para>

    <sect1 id="HPCC_Platform">
      <title>The HPCC Platform</title>

      <para>HPCC is fast, flexible and highly scalable. It can be used for any
      data-centric task and can meet the needs of any database regardless of
      size. There are two types of cluster:</para>

      <itemizedlist>
        <listitem>
          <para>The Thor cluster is used to process all data in all
          files.</para>
        </listitem>

        <listitem>
          <para>The Roxie cluster is used to search for a particular record or
          set of records.</para>
        </listitem>
      </itemizedlist>

      <para>As shown by the following diagram, the HPCC architecture also
      incorporates:</para>

      <itemizedlist>
        <listitem>
          <para>Common middle-ware components.</para>
        </listitem>

        <listitem>
          <para>An external communications layer.</para>
        </listitem>

        <listitem>
          <para>Client interfaces which provide both end-user services and
          system management tools.</para>
        </listitem>

        <listitem>
          <para>Auxiliary components to support monitoring and to facilitate
          loading and storing of file system data from external sources</para>
        </listitem>
      </itemizedlist>

      <para><graphic fileref="images/ML008.jpg" /></para>

      <para>For more information about the HPCC architecture, clusters and
      components, see the HPCC website, <ulink
      url="http://hpccsystems.com/Why-HPCC/How-it-works">http://hpccsystems.com/Why-HPCC/How-it-works</ulink>.</para>
    </sect1>

    <sect1 id="ECL_Programming_Language">
      <title id="ECL_Programming_Lang">The ECL Programming Language</title>

      <para>The ECL programming language is a key factor in the flexibility
      and capabilities of the HPCC processing environment. It is designed to
      be a transparent and implicitly parallel programming language for
      data-intensive applications. It is a high-level, highly-optimized,
      data-centric declarative language that allows programmers to define what
      the data processing result should be and the dataflows and
      transformations that are necessary to achieve the result.</para>

      <para>Execution is not determined by the order of the language
      statements, but from the sequence of dataflows and transformations
      represented by the language statements. It combines data representation
      with algorithm implementation, and is the fusion of both a query
      language and a parallel data processing language.</para>

      <para>ECL uses an intuitive syntax which has taken cues from other
      familiar languages, supports modular code organization with a high
      degree of reusability and extensibility, and supports high-productivity
      for programmers in terms of the amount of code required for typical
      applications compared to traditional languages like Java and C++. It is
      compiled into optimized C++ code for execution on the HPCC system
      platform, and can be used for complex data processing and analysis jobs
      on a Thor cluster or for comprehensive query and report processing on a
      Roxie cluster.</para>

      <para>ECL allows inline C++ functions to be incorporated into ECL
      programs, and external programs in other languages can be incorporated
      and parallelized through a PIPE facility.</para>

      <para>External services written in C++ and other languages which
      generate DLLs can also be incorporated in the ECL system library, and
      ECL programs can access external Web services through a standard
      SOAPCALL interface.</para>

      <para>The ECL language includes extensive capabilities for data
      definition, filtering, data management, and data transformation, and
      provides an extensive set of built-in functions to operate on records in
      datasets which can include user-defined transformation functions. The
      Thor system allows data transformation operations to be performed either
      locally on each node independently in the cluster, or globally across
      all the nodes in a cluster, which can be user-specified in the ECL
      language.</para>

      <para>An additional important capability provided in the ECL programming
      language is support for natural language processing (NLP) with PATTERN
      statements and the built-in PARSE operation. Using this capability of
      the ECL language it is possible to implement parallel processing from
      information extraction applications across document files including
      XML-based documents or Web pages.</para>

      <para>Some benefits of using ECL are:</para>

      <itemizedlist>
        <listitem>
          <para>It incorporates transparent and implicit data parallelism
          regardless of the size of the computing cluster and reduces the
          complexity of parallel programming increasing development
          productivity.</para>
        </listitem>

        <listitem>
          <para>It enables the implementation of data-intensive applications
          with huge volumes of data previously thought to be intractable or
          infeasible. ECL was specifically designed for manipulation of data
          and query processing. Orders of magnitude performance increases over
          other approaches are possible.</para>
        </listitem>

        <listitem>
          <para>The ECL compiler generates highly optimized C++ for
          execution.</para>
        </listitem>

        <listitem>
          <para>It is a powerful, high-level, parallel programming language
          ideal for implementation of ETL, information retrieval, information
          extraction, record linking and entity resolution, and many other
          data-intensive applications.</para>
        </listitem>

        <listitem>
          <para>It is a mature and proven language but is still evolving as
          new advancements in parallel processing and data intensive computing
          occur.</para>
        </listitem>
      </itemizedlist>

      <para>The HPCC platform also provides a comprehensive IDE (ECL IDE)
      which provide a highly interactive environment for rapid development and
      implementation of ECL applications.</para>

      <para>HPCC and the ECL IDE downloads are available from the HPCC systems
      website, <ulink
      url="http://hpccsystems.com/">http://hpccsystems.com/</ulink> which also
      provides access to documentation and tutorials.</para>
    </sect1>

    <sect1 id="ECL_IDE">
      <title>ECL IDE</title>

      <para>ECL IDE is an ECL programmer's tool. Its main use is to create
      queries and ECL files and is designed to make ECL coding as easy as
      possible. It has all the ECL built-in functions available to you for
      simple point-and-click use in your query construction. For example, the
      Standard String Library (Std.Str) contains common functions to operate
      on STRING fields such as the ToUpperCase function which converts
      characters in a string to uppercase.</para>

      <para>You can mix-and-match your data with any of the ECL built-in
      functions and/or ECL files you have defined to create Queries. Because
      ECL files build upon each other, the resulting queries can be as complex
      as needed to obtain the result.</para>

      <para>Once the Query is built, submit it to an HPCC cluster, which will
      process the query and return the results.</para>

      <para>Configuration files (.CFG) are used to store the information for
      any HPCC you want to connect to, for example, it stores the location of
      the HPCC and the location of any folders containing ECL files that you
      may want to use while developing queries. These folders and files are
      shown in the <emphasis role="bold">Repository</emphasis> window.</para>

      <para><graphic fileref="images/CT06.jpg" /></para>

      <para>For more information on using ECL IDE see the Client Tools manual
      which may be downloaded from the HPCC website, <ulink
      url="http://hpccsystems.com/community/docs/client-tools">http://hpccsystems.com/community/docs/client-tools</ulink>.</para>
    </sect1>

    <sect1 id="Installing_Using_ML">
      <title id="Installing_Using_Libraries">Installing and using the ML
      Libraries</title>

      <para>The ML Libraries can only be used in conjunction with an HPCC
      System, ECL IDE and the Client tools.</para>

      <sect2>
        <title id="Requirements">Requirements</title>

        <para>If you don't already use the HPCC platform and/or ECL IDE and
        the Client Tools, you must download and install them before
        downloading the ML libraries:</para>

        <itemizedlist>
          <listitem>
            <para>Download and install the relevant HPCC platform for your
            needs. (<ulink
            url="http://hpccsystems.com/download/free-community-edition">http://hpccsystems.com/download/free-community-edition</ulink>)</para>
          </listitem>

          <listitem>
            <para>Download and install the ECL IDE and Client Tools. (<ulink
            url="http://hpccsystems.com/download/free-community-edition/ecl-ide-and-client-tools">http://hpccsystems.com/download/free-community-edition/ecl-ide-and-client-tools</ulink>)</para>
          </listitem>
        </itemizedlist>

        <para>The ML Libraries can also be used on an HPCC Systems
        One-Click<superscript>TM</superscript> Thor, which is available to
        anyone with an Amazon AWS account. To walk-through an example of how
        to use the One-Click<superscript>TM</superscript> Thor with the
        Machine Learning Libraries, see the <emphasis>Associations
        (ML.Assocaite)</emphasis> section in <emphasis>The ML
        Module</emphasis> chapter later in this manual.</para>

        <para>To setup a One-Click<superscript>TM</superscript> Thor
        cluster:</para>

        <itemizedlist>
          <listitem>
            <para>Setup an Amazon AWS account (<ulink
            url="http://aws.amazon.com/account/">http://aws.amazon.com/account/</ulink>)</para>
          </listitem>

          <listitem>
            <para>Login and Launch your Thor cluster. The <emphasis
            role="bold">Login</emphasis> button on the HPCC Systems website
            (<ulink url="???"><ulink
            url="https://aws.hpccsystems.com/aws/getting_started/">https://aws.hpccsystems.com/aws/getting_started/</ulink></ulink>),
            provides an automated setup process which is quick and easy to
            use.</para>
          </listitem>

          <listitem>
            <para>Download the ECL IDE and Client Tools onto your computer
            ((<ulink
            url="http://hpccsystems.com/download/free-community-edition/ecl-ide-and-client-tools">http://hpccsystems.com/download/free-community-edition/ecl-ide-and-client-tools</ulink>))</para>
          </listitem>
        </itemizedlist>

        <para>If you are new to the ECL Language, take a look at the a
        programmers guide and language reference guides, <ulink
        url="http://hpccsystems.com/community/docs/learning-ecl">http://hpccsystems.com/community/docs/learning-ecl</ulink>.</para>

        <para>The HPCC Systems website also provides tutorials designed to get
        you started using data on the HPCC System, <ulink
        url="http://hpccsystems.com/community/docs/tutorials">http://hpccsystems.com/community/docs/tutorials</ulink>.</para>
      </sect2>

      <sect2>
        <title id="Intsalling_ML_Libraries">Installing the ML
        Libraries</title>

        <para>To install the ML Libraries:</para>

        <orderedlist>
          <listitem>
            <para>Go to the Machine Learning page of the HPCC Systems website,
            <ulink
            url="http://hpccsystems.com/ml">http://hpccsystems.com/ml</ulink>
            and click on <emphasis role="bold">Download and Get
            Started</emphasis>.</para>
          </listitem>

          <listitem>
            <para>Click on <emphasis role="bold">Step 1: Download the ML
            Library</emphasis> and save the file to your computer.</para>
          </listitem>

          <listitem>
            <para>Extract the downloaded files to your ECL IDE source folder.
            This folder is typically located here:
            "C:\Users\Public\Documents\HPCC Systems\ECL\My Files".</para>
          </listitem>
        </orderedlist>

        <para><emphasis role="bold">Note:</emphasis> To find out the location
        of your <emphasis role="bold">Working Folder</emphasis>, simply go to
        your ECL IDE <emphasis role="bold">Preferences</emphasis> window
        either from the login dialog or from the <emphasis
        role="bold">Orb</emphasis> menu. Click on the <emphasis
        role="bold">Compiler</emphasis> tab and use the first <emphasis
        role="bold">Working Folder</emphasis> location listed.</para>

        <para>The ML Libraries are now ready to be used. To locate them,
        display the <emphasis role="bold">Repository Window</emphasis> in ECL
        IDE and expand the <emphasis role="bold">My Files</emphasis> folder to
        see the <emphasis role="bold">ML</emphasis> folder.</para>
      </sect2>

      <sect2>
        <title id="Using_ML_Libraries">Using the ML Libraries</title>

        <para>A walk-through is provided for all Machine Learning Libraries
        supported, which are designed to get you started using ML with the
        HPCC System. Each module is also covered in this manual in a separate
        section which contains more detailed information about the
        functionality of the routines included.</para>

        <para>To use the ML Libraries, you also need to upload some data onto
        the <emphasis role="bold">Dropzone</emphasis> of your cluster. If you
        already have a file on your computer, you can upload it onto the
        <emphasis role="bold">Dropzone</emphasis> using <emphasis
        role="bold">ECL Watch</emphasis>. Simply, use the <emphasis
        role="bold">DFU Files/Upload/download</emphasis> menu item, locate the
        file(s), select and upload.</para>

        <para>Now that the ML Libraries are installed and you have uploaded
        your data, you can use ECL IDE to write queries to analyze your
        data:</para>

        <orderedlist>
          <listitem>
            <para>Login to ECL IDE, accessing the HPCC System you have
            installed.</para>
          </listitem>

          <listitem>
            <para>Using the <emphasis role="bold">Repository</emphasis>
            toolbox, expand <emphasis role="bold">My Files</emphasis>.</para>
          </listitem>

          <listitem>
            <para>Expand the <emphasis role="bold">ML</emphasis> folder to
            locate the Machine Learning files you want to use.</para>
          </listitem>

          <listitem>
            <para>Open a new builder window and start writing your query. To
            reference the ML libraries in your ECL source code, use an import
            statement. For example:<programlisting>IMPORT * FROM ML;
IMPORT * FROM ML.Cluster;
IMPORT * FROM ML.Types;

//Define my record layout
MyRecordLayout := RECORD
UNSIGNED RecordId;
REAL XCoordinate;
REAL YCoordinate;
END;

//My dataset
X2 := DATASET([      
{1, 1, 5},
{2, 5, 7},
{3, 8, 1},
{4, 0, 0},
{5, 9, 3},
{6, 1, 4},
{7, 9, 4}], MyRecordLayout);

//Three candidate centroids
CentroidCandidates := DATASET([
{1, 1, 5},
{2, 5, 7},
{3, 9, 4}], MyRecordLayout);

//Convert them to our internal field format
ml.ToField(X2, fX2);
ml.ToField(CentroidCandidates, fCentroidCandidates);

//Run K-Means for, at most, 10 iterations and stop if delta &lt; 0.3 between iterations
fX3 := Kmeans(fX2, fCentroidCandidates, 10, 0.3);

//Convert the final centroids to the original layout
ml.FromField(fX3.result(), MyRecordLayout, X3);

//Display the results
OUTPUT(X3);
</programlisting></para>
          </listitem>
        </orderedlist>
      </sect2>

      <sect2>
        <title id="Contributing_Sources">Contributing to the sources</title>

        <para>Both HPCC and ECL-ML are open source projects and contributions
        to the sources are welcome. If you are interested in contributing to
        these projects, simply download the GitHub client and go to the
        relevant GitHub pages.</para>

        <itemizedlist>
          <listitem>
            <para>To contribute to the HPCC open source project, go to <ulink
            url="https://github.com/hpcc-systems/HPCC-Platform">https://github.com/hpcc-systems/HPCC-Platform</ulink>.</para>
          </listitem>

          <listitem>
            <para>To contribute to the ECL-ML open source project, go to
            <ulink
            url="https://github.com/hpcc-systems/ecl-ml">https://github.com/hpcc-systems/ecl-ml</ulink>.</para>
          </listitem>
        </itemizedlist>

        <para>You are required to sign a contribution agreement to become a
        contributor.</para>
      </sect2>
    </sect1>
  </chapter>

  <chapter id="Machine_Learning_Algorithms">
    <title>Machine Learning Algorithms</title>

    <para>The HPCC Systems Machine Learning libraries contain an extensible
    collection of machine learning routines which are easy and efficient to
    use and are designed to execute in parallel across a cluster. T</para>

    <para>he list of modules supported will continue to grow over time. The
    following modules are currently supported:</para>

    <itemizedlist>
      <listitem>
        <para>Associations (ML.Associate)</para>
      </listitem>

      <listitem>
        <para>Classify (ML.Classify)</para>
      </listitem>

      <listitem>
        <para>Cluster (ML.Cluster)</para>
      </listitem>

      <listitem>
        <para>Correlations (ML.Correlate)</para>
      </listitem>

      <listitem>
        <para>Discretize (ML.Discretize)</para>
      </listitem>

      <listitem>
        <para>Distribution (ML.Distribution)</para>
      </listitem>

      <listitem>
        <para>Field Aggregates (ML.FieldAggregates)</para>
      </listitem>

      <listitem>
        <para>Regression (ML.Regression)</para>
      </listitem>

      <listitem>
        <para>Visualization (ML.VL)</para>
      </listitem>
    </itemizedlist>

    <para>The Machine Learning modules are supported by the following which
    are also used to implement ML:</para>

    <itemizedlist>
      <listitem>
        <para>The Matrix Library (Mat)</para>
      </listitem>

      <listitem>
        <para>Utility (ML.Utility)</para>
      </listitem>

      <listitem>
        <para>Docs (ML.Doc)</para>
      </listitem>
    </itemizedlist>

    <para>The ML Modules are used in conjunction with the HPCC system. More
    information about the HPCC System is available on the following website,
    <ulink url="http://hpccsystems.com/"><ulink
    url="http://hpccsystems.com/">http://hpccsystems.com/</ulink></ulink>.</para>

    <sect1 id="ML_Data_Models">
      <title>The ML Data Models</title>

      <para>The ML routines are all centered around a small number of core
      processing models. As a user of ML (rather than an implementer) the
      exact details of these models can generally be ignored. However, it is
      useful to have some idea of what is going on and what routines are
      available to help you with the various models. The formats that are
      shared between various modules within ML are all contained within the
      Type definition.</para>

      <sect2 id="Numeric_Field">
        <title>Numeric field</title>

        <para>The principle type that undergirds most of the ML processing is
        the Numeric Field. This is a general representation of an arbitrary
        ECL record of numeric entries. The record has 3 fields:</para>

        <informaltable>
          <tgroup cols="2">
            <colspec align="left" colwidth="100pt" />

            <colspec align="left" colwidth="400" />

            <thead>
              <row>
                <entry align="left">Field</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Id</entry>

                <entry>The 'record' id. This is an identifier for the record
                being modeled. It will be shared between all of the fields of
                the record.</entry>
              </row>

              <row>
                <entry>Field Number</entry>

                <entry>And ECL record with 10 fields produces 10
                ‘numericfield’ records, one with each of the field numbers
                from 1 to 10<superscript>[1]</superscript>.</entry>
              </row>

              <row>
                <entry>Value</entry>

                <entry>The value of the field.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>

        <para>This is perhaps visualized by comparison to a traditional ECL
        record. Here is a simple example showing some height, weight and age
        facts for certain individuals:<programlisting>IMPORT ml;

value_record := RECORD
                UNSIGNED rid;
                REAL height;
                REAL weight;
                REAL age;
                INTEGER1 species; // 1 = human, 2 = tortoise
                INTEGER1 gender; // 0 = unknown, 1 = male, 2 = female
END;


d := dataset([{1,5*12+7,156*16,43,1,1},
              {2,5*12+7,128*16,31,1,2},
              {3,5*12+9,135*16,15,1,1},
              {4,5*12+7,145*16,14,1,1},
              {5,5*12-2,80*16,9,1,1},
              {6,4*12+8,72*16,8,1,1},
              {7,8,32,2.5,2,2},
              {8,6.5,28,2,2,2},
              {9,6.5,28,2,2,2},
              {10,6.5,21,2,2,1},
              {11,4,15,1,2,0},
              {12,3,10.5,1,2,0},
              {13,2.5,3,0.8,2,0},
              {14,1,1,0.4,2,0}
              ]
              ,value_record);


d;
</programlisting></para>

        <para>It has 14 rows of data. Each row has 5 interesting data fields
        and a <emphasis>record id</emphasis> that is prepended to uniquely
        identify the record. Therefore a <emphasis>5 field ECL
        record</emphasis> actually has 6 fields.</para>

        <para>ML provides the ToField operation that converts a record in this
        general format to the NumericField format. Thus:</para>

        <programlisting>ml.ToField(d,o);
d;
o
</programlisting>

        <para>Shows not only the original data, but also the data in the
        standard ML NumericField format. The latter has 70 rows (5x14).
        Incidentally: ToField is an example of a macro that uses a 'out'
        parameter (o) rather than returning a value. If a file has N rows and
        M columns then the order of the ToField operation will be O(mn). It is
        also possible to turn the NumericField format back into a
        <emphasis>regular</emphasis> ECL style record using the FromField
        operation: <programlisting>ml.ToField(d,o);
d;
o; 
ml.FromField(o,value_record,d1); 
d1; 
</programlisting></para>

        <para>Will leave d1 = d.</para>

        <sect3>
          <title>Advanced - Converting more complex records</title>

          <para>By default, the ToField operation assumes the first field is
          the “id” field, and all subsequent numeric fields are to be assigned
          a field number in the resulting table. However, additional
          parameters may be specified to ToField that facilitates the ability
          to specify the name of the id column in the original table as well
          as the columns to be used as data fields. For example:</para>

          <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
value_record := RECORD
STRING first_name;
STRING last_name;
UNSIGNED name_id;
REAL height;
REAL weight;
REAL age;
STRING eye_color;
INTEGER1 species; // 1 = human, 2 = tortoise
INTEGER1 gender; // 0 = unknown, 1 = male, 2 = female
END;

dOrig := dataset([
{'Charles','Babbage',1,5*12+7,156*16,43,'Blue',1,1},
{'Tim','Berners-Lee',2,5*12+7,128*16,31, 'Brown',1,1},
{'George','Boole',3,5*12+9,135*16,15, 'Hazel',1,1},
{'Herman','Hollerith',4,5*12+7,145*16,14,'Green',1,1},
{'John','Von Neumann',5,5*12-2,80*16,9,'Blue',1,1},
{'Dennis','Ritchie',6,4*12+8,72*16,8, 'Brown',1,1},
{'Alan','Turing',7,8,32,2.5, 'Brown',2,1}
],value_record);

ML.ToField(dOrig,dResult,name_id,'height,weight,age,gender');
dOrig;
dResult;

</programlisting>In the above example, the name_id column is taken as the id.
          Height, weight, age and gender will be parsed into numbered
          fields.</para>

          <para><emphasis role="bold">Note:</emphasis> The id name is not in
          quotes, but the comma-delimited list of fields is.</para>

          <para>Along with creating the new table in NumericField format, the
          ToField macro also creates three other objects to help with field
          translation, two functions and a dataset.</para>

          <para>The two functions are outtable_ToName() and
          outtable_ToNumber(), where outtable is the name of the output table
          specified in the macro call. Passing in a number in the first one
          will produce the field name mapped to that number, and passing a
          string into the second one will produce the number assigned to that
          field name.</para>

          <para>For the previous example, we can therefore do the
          following:</para>

          <para><programlisting><?dbfo keep-together="always"?>dResult_ToName(2);      // Returns ‘weight’
dResult_ToNumber(‘age’) // Returns 3 (note that the field name is always lowercase)
</programlisting></para>

          <para>The other dataset that is created is a 2-column mapping table
          named outtable_Map which contains every field from the original
          table in the first column, and what it is mapped to in the second
          column.</para>

          <para>This would either be the column number, the string “ID” if it
          is the ID field, or the string “NA” indicating that the field was
          not mapped to a NumericField number. In the above example, the table
          is named:</para>

          <para><programlisting><?dbfo keep-together="always"?>dResult_Map;
</programlisting></para>

          <para>The mapping table may be used when reconstituting the data
          back to the original format. For example:</para>

          <para><programlisting><?dbfo keep-together="always"?>ML.FromField(dResult,value_record,dReconstituted,dResult_Map);
dReconstituted;
</programlisting></para>

          <para>The output from this FromField call will have the same
          structure as the initial table, and values that existed in the
          NumericField version of the table will be allocated to the fields
          specified in the mapping table.</para>

          <para><emphasis role="bold">Note:</emphasis> Any data that did not
          translate into the NumericField table will be left blank or zero in
          the reconstituted table.</para>
        </sect3>
      </sect2>

      <sect2 id="Discrete_field">
        <title id="Discrete_Field">Discrete field</title>

        <para>Some of the ML routines do not require the field values to be
        real, rather they require discrete (integral) values. The structure of
        the records are essentially identical to NumericField but, the value
        is of type t_Discrete (typically INTEGER) rather than t_FieldReal
        (typically REAL8).</para>

        <para>There are no explicit routines to get to a discrete-field
        structure from an ECL record, rather it is presumed that NumericField
        will be used as an intermediary.</para>

        <para>There is an entire module (Discretize) devoted to moving a
        NumericField structured file into a DiscreteField structured file. The
        options and reasons for the options are described in the Discretize
        module section.</para>

        <para>For this introduction it is adequate to show that all of the
        numeric fields could be made integral simply by using:</para>

        <para><programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);
o;
o1 := ML.Discretize.ByRounding(o);
o1</programlisting></para>
      </sect2>

      <sect2 id="ItemElement">
        <title id="Item_element">ItemElement</title>

        <para>A rather more specialist format is the ItemElement format. This
        does not model an ECL record directly, rather it models an abstraction
        that can be derived from an ECL record.</para>

        <para>The item element has a record id and a value (which is of type
        t_Item). The t_Item is an integral value – but unlike t_Discrete the
        values are not considered to be ordinal. Put another way, in
        t_Discrete 4 &gt; 3 and 2 &lt; 3. In t_Item the 2, 3, 4 are just
        arbitrary labels that ‘happen’ to be integers for efficiency.</para>

        <para><emphasis role="bold">Note:</emphasis> ItemElement does not have
        a field number.</para>

        <para>There is no significance placed upon the field from which the
        value was derived. This models the abstract notion of a collection of
        ‘bags’ of items. An example of the use of this type of structure will
        be given in the Using ML with documents section.</para>
      </sect2>

      <sect2 id="Coding_ML_Data_Models" role="nobrk">
        <title>Coding with the ML data models</title>

        <para>The ML data models are extremely flexible to work with; but
        using them is a little different from traditional ECL programming.
        This section aims to detail some of the possibilities.</para>

        <sect3 id="Column_Splitting">
          <title>Column splitting</title>

          <para>Some of the ML routines expect to be handed two datasets which
          may be, for example, a dataset of independent variables and another
          of dependent variables. The data as it originally exists will
          usually have the independent and dependent data within the same row.
          For example, when using a classifier to produce a model to predict
          the species or gender of an entity from the other details, the
          height, weight and age fields would need to be in a different ‘file’
          to the species and gender. However, they have to have the same
          record ID to show the correlation between the two. In the ML data
          model this is as simple as applying two filters: <programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);
o1 := ML.Discretize.ByBucketing(o,5);
Independents := o1(Number &lt;= 3);
Dependents := o1(Number &gt;= 4);
Bayes := ML.Classify.BuildNaiveBayes(Independents,Dependents);
Bayes
</programlisting></para>
        </sect3>

        <sect3 id="Genuine_Nulls">
          <title>Genuine nulls</title>

          <para>Implementing a genuine null can be done by simply removing
          certain fields with certain values from the datastream. For example,
          if 0 was considered an invalid weight then one could
          do:<programlisting><?dbfo keep-together="always"?>Better := o(Number&lt;&gt;2 OR Value&lt;&gt;0);</programlisting></para>
        </sect3>

        <sect3 id="Sampling">
          <title>Sampling</title>

          <para>By far the easiest way to split a single data file into
          samples is to use the SAMPLE and ENTH verbs upon the datafile PRIOR
          to the conversion to ML format.</para>
        </sect3>

        <sect3 id="Inserting_Column_Computed_Value">
          <title id="Inserting_column">Inserting a column with a computed
          value</title>

          <para>Inserting a column with a new value computed from another
          field value is a fairly advanced technique. The following inserts
          the square of the weight as a new column:<programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);

BelowW := o(Number &lt;= 2); 
// Those columns whose numbers are not changed
// Shuffle the other columns up - this is not needed if appending a column
AboveW := PROJECT(o(Number&gt;2),TRANSFORM(ML.Types.NumericField,SELF.Number := 
LEFT.Number+1, SELF := LEFT));
NewCol := PROJECT(o(Number=2),TRANSFORM(ML.Types.NumericField,
                                                        SELF.Number := 3,
                                                        SELF.Value := LEFT.Value*LEFT.Value,
                                                        SELF := LEFT) );
    
NewO := BelowW+AboveW+NewCol;

NewO;
</programlisting></para>
        </sect3>
      </sect2>
    </sect1>

    <sect1 id="Generating_test_data">
      <title id="Generating_Test_Data">Generating test data</title>

      <para>ML is interesting when it is being executed against data with
      meaning and significance. However, sometimes it can be useful to get
      hold of a lot of data quickly for testing purposes. This data may be
      ‘random’ (by some definition) or it may follow a number of carefully
      planned statistical distributions. The ML libraries have support for
      high performance ‘random value’ generation using the GenData command
      inside the distribution module.</para>

      <para>GenData generates one column at a time although it generates that
      column for all the records in the file. It works in parallel so is very
      efficient.</para>

      <para>The easiest type of column to generate is one in which the values
      are evenly and randomly distributed over a range. The following
      generates 1M records each with a random number from 0-100 in the first
      column:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

TestSize := 1000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform

</programlisting></para>

      <para>To generate 1M records with three columns; one Uniformly
      distributed, one Normally distributed (mean 0, Standard Deviation 10)
      and one with a Poisson distribution (Mean of 4):</para>

      <programlisting><?dbfo keep-together="always"?>IMPORT ML;

TestSize := 1000000;

a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
// Field 2 Normally Distributed
a2 := ML.Distribution.Normal2(0,10,10000);
b2 := ML.Distribution.GenData(TestSize,a2,2);
// Field 3 - Poisson Distribution
a3 := ML.Distribution.Poisson(4,100);
b3 := ML.Distribution.GenData(TestSize,a3,3);

D := b1+b2+b3; // This is the test data

ML.FieldAggregates(D).Simple;  // Perform some statistics on the test data to ensure 
                                  it worked
</programlisting>

      <para>This generates the data in the correct format and even produces
      some statistics to ensure it works!</para>

      <para>The ML libraries have over half a dozen different distributions
      that the generated data columns can be given. These are described at
      length in the Distribution module section.</para>
    </sect1>
  </chapter>

  <chapter id="ML_module_walkthroughs">
    <title id="ML_Module_Walktrhoughs">ML module walk-throughs</title>

    <para>To help you get started, a walk-through is provided for each ML
    module. The walk-throughs explain how the modules work and demonstrate how
    they can be used to generate the results you require.</para>

    <sect1>
      <title id="Association_Walkthrough">Association walk-through</title>

      <para>Association mining is one of the most wide-spread, if not widely
      known forms of machine learning. If you have ever entered a few items
      into an online ‘shopping-basket’ and then been prompted to buy more
      things, which were exactly what you wanted, then the chances are there
      was an association miner working in the background.</para>

      <para>At their simplest, association mining algorithms are handed a
      large number of ‘collections of items’ and then they find which items
      co-occur in most of the collections. The ECL-ML association algorithms
      are used by instantiating an Association module passing in a dataset of
      Items and a number, which is the minimum number of co-occurrences
      considered to be significant (the lower this number, the slower the
      algorithm).</para>

      <para>The following code creates such a module using data which is
      randomly generated but tweaked to have some relationships within
      it.</para>

      <para><programlisting>IMPORT ML;
TestSize := 100000;
CoOccurs := TestSize/1000;

a1 := ML.Distribution.Poisson(5,100); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
a3 := ML.Distribution.Poisson(3,100);
b3 := ML.Distribution.GenData(TestSize,a3,3);

D := b1+b2+b3; // This is the test data
// Now construct a fourth column which is a function of column 1
B4 := PROJECT(b1,TRANSFORM(ML.Types.NumericField, SELF.Number:=4, SELF.Value:=LEFT.Value * 2, 
      SELF.Id := LEFT.id));

AD0 := PROJECT(ML.Discretize.ByRounding(B1+B2+B3+B4),ML.Types.ItemElement);
// Remove duplicates from bags (fortunately the generation allows this to be local)
AD := DEDUP( SORT( AD0, ID, Value, LOCAL ), ID, Value, LOCAL );

ASSO := ML.Associate(AD,CoOccurs);
</programlisting>The simplest question which can now be asked is: “Which pairs
      of items are most likely to appear together?”. The following provides
      the answer:</para>

      <programlisting>TOPN(Asso.Apriori2,50,-Support)</programlisting>

      <para>The answer to: ‘Which triplets can be found together?’ is answered
      by:</para>

      <programlisting>TOPN(Asso.Apriori3,50,-Support);</programlisting>

      <para>The same answer can also be found by asking the question:</para>

      <programlisting>TOPN(Asso.EclatN(3,3),50,-Support);</programlisting>

      <para>The second form uses a different algorithm for the computation
      (Eclat), which also has a more flexible interface. The first parameter
      gives the maximum group size that is interesting. The second parameter
      gives the minimum group size that is interesting.</para>

      <para>Thus to find the largest groups of size 4 or 3 use:</para>

      <programlisting>TOPN(Asso.EclatN(4,3),50,-Support);</programlisting>

      <para>It may be noted that there is also an AprioriN function. However
      it relies upon a feature that is not currently functional in version
      3.4.2 of the HPCC platform.</para>

      <para>In addition to being able to spot the common patterns, the
      association module is able to turn a set of patterns into a set of rules
      or to use a different term of art, it is capable of building a
      predictor. Essentially a predictor answers the question: “Given I have
      this in my basket, what will come next.?”. A predictor is built by
      passing the output of EclatN or ApriorN into the Rules function:</para>

      <programlisting>R := TOPN(Asso.EclatN(4,2),50,-Support);

Asso.Rules(R)
</programlisting>

      <para>This produces the following numbers:<graphic
      fileref="images/ML009.jpg" /></para>

      <para><emphasis role="bold">Note:</emphasis> Your numbers will be
      different because the data is randomly generated when run.</para>

      <para>The support tells you how many patterns were used to make the
      prediction.</para>

      <para>Conf tells the percentage of times that the prediction would be
      correct (in the training data, but real life might be
      different!).</para>

      <para>Sig is used to indicate whether the next item is likely to be
      causal (high sig) or co-incidental (low-sig). To understand the
      difference, imagine that you go into Best Buy to buy an expensive
      telephone. Your shopping basket of 1 item will probably allow the system
      to predict two different likely next items, such as a case for the phone
      and a candy bar. They might both have high confidence, but the case will
      have high significance (you will usually by the case if you by the
      phone), the candy will not (it is only likely because ‘everyone buys
      candy’).</para>
    </sect1>

    <sect1 id="Classify_walkthrough">
      <title id="Classification_Walkthrough">Classification
      walk-through</title>

      <para><emphasis role="bold">Modules: Classify</emphasis></para>

      <para>ML.Classify tackles the problem: “given I know these facts about
      an object; can I predict some other value or attribute of that object.”
      This is really where data processing gives way to machine learning:
      based upon some form of training set can I derive a rule or model to
      predict something something about other data records.</para>

      <para>Classification is sufficiently central to machine learning that we
      provide four different methods of doing it. You will need to examine the
      literature or experiment to decide exactly which method of
      classification will work best in any given context. In order to simplify
      coding and to allow experimentation all of our classifiers can be used
      through the unified classifier interface. <emphasis></emphasis></para>

      <para><emphasis>Using</emphasis> a classifier in ML can be viewed as
      three logical steps:</para>

      <orderedlist>
        <listitem>
          <para>Learning the model from a training set of data that has been
          classified externally.</para>
        </listitem>

        <listitem>
          <para>Testing. Getting measures of how well the classifier
          fits.</para>
        </listitem>

        <listitem>
          <para>Classifying. Apply the classifier to new data in order to give
          it a classification.</para>
        </listitem>
      </orderedlist>

      <para>In the examples that follow we are simply trying to show how a
      given method can be used in a given context; we are not necessarily
      claiming it is the best or only way to solve the given example.</para>

      <para>A classifier will not predict anything if handed totally random
      data. It is precisely looking for a relationship between the data that
      is non-random. So this example will generate test data by:</para>

      <orderedlist>
        <listitem>
          <para>Generating three random columns.</para>
        </listitem>

        <listitem>
          <para>Producing a fourth column that is the sum of the three
          columns.</para>
        </listitem>

        <listitem>
          <para>Giving the fourth column a category from 0 (small) to 2
          (big).</para>
        </listitem>
      </orderedlist>

      <para>The object is to see if the system can learn to predict from the
      individual fields which category the record will be assigned. The data
      generation is a little more complex than normal so it is presented here
      (this code is also available in Tests/Explanatory/Classify in our source
      distribution).</para>

      <programlisting><?dbfo keep-together="always"?>IMPORT ML;
// First auto-generate some test data for us to classify

TestSize := 100000;
a1 := ML.Distribution.Poisson(5,100); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
a3 := ML.Distribution.Poisson(3,100);
b3 := ML.Distribution.GenData(TestSize,a3,3);

D := b1+b2+b3; // This is the test data

// Now construct a fourth column which is the sum of them all
B4 := PROJECT(TABLE(D,{Id,Val := SUM(GROUP,Value)},Id),TRANSFORM(ML.Types.NumericField,
                                           SELF.Number:=4,
                                           SELF.Value:=MAP(LEFT.Val &lt; 6 =&gt; 0,  // Small
                                                           LEFT.Val &lt; 10 =&gt; 1, // Normal
                                                                          2 ); // Big
                                           SELF := LEFT));
D1 := D+B4;
</programlisting>

      <para></para>

      <para>The data generated for D1 is in numeric field format which is to
      say that the variables are continuous (real numbers). Classifiers
      require that the ‘target’ results (the numbers to be produced) are
      positive integers. The unified classier interface allows for the inputs
      to a classifier to be either continuous or discrete (using the ‘C’ and
      ‘D’ versions of the functions). However, most of the implemented
      classifiers prefer discrete input and you get better control over the
      discretization process if you do it yourself. The Discretize module can
      do this (see the section ML Data Models and the Discretize module which
      explain this in more detail) but to keep things simple we will just
      round all the fields to integers:</para>

      <programlisting><?dbfo keep-together="always"?>// We are going to use the 'discrete' classifier interface, so discretize our data first
D2 := ML.Discretize.ByRounding(D1);</programlisting>

      <para>In the rest of this section if a ‘D2’ appears from 'nowhere', it
      is referencing this dataset.</para>

      <para>Every classifier has a module within the classify module. It is
      usually worth grabbing hold of that first to make the rest of the typing
      simpler. In this case we will get the NaiveBayes module:</para>

      <programlisting>BayesModule := ML.Classify.NaiveBayes;</programlisting>

      <para>While I labeled it ‘BayesModule’, it important to understand that
      that one line is the only difference between whether you are using
      NaiveBayes, Perceptrons, LogisticRegression or one of the other
      classifier mechanisms.</para>

      <para>For illustration purposes we will skip straight to testing:</para>

      <programlisting>TestModule := BayesModule.TestD(D2(Number&lt;=3),D2(Number=4));
TestModule.Raw;
TestModule.CrossAssignments;
TestModule.PrecisionByClass;
TestModule.Headline;
</programlisting>

      <para>The TestModule := does all the work.</para>

      <para>Firstly note that D2 has been split into two pieces. The first
      parameter is all of the independent variables (sometimes called
      features), the second parameter is the dependent variables (or classes).
      The fact that TestD was called (rather than TestC) is to indicate that
      the independent variables are discrete.</para>

      <para>The module now has four different outputs to show you how well the
      classification worked:</para>

      <informaltable>
        <tgroup cols="2">
          <colspec align="left" colwidth="110pt" />

          <colspec align="left" colwidth="480pt" />

          <thead>
            <row>
              <entry align="left">Result</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry align="left">Headline</entry>

              <entry>Gives you the main precision number. On this test data,
              the result shows how often the classifier was correct.</entry>
            </row>

            <row>
              <entry align="left">PrecisionByClass</entry>

              <entry>Similar to Headline except that it gives the precision
              broken down by the class that it SHOULD have been classified to.
              It is possible that a classifier might work well in general but
              may be particularly poor at identifying one of the
              groups.</entry>
            </row>

            <row>
              <entry align="left">CrossAssignments</entry>

              <entry>It is one thing to say a classification is ‘wrong’. This
              table shows, “if a particular class is mis-classified, what is
              it most likely to be mis-classified as?”.</entry>
            </row>

            <row>
              <entry align="left">Raw</entry>

              <entry>Gives a very detailed breakdown of every record in the
              test corpus, for example, what the classification should have
              been and what it was.</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para>Assuming you like the results you will normally learn the model
      and then use it for classification. In the ‘real world’ you would
      probably do the learning and the classifying at very different times and
      on very different data. For illustration purposes and simplicity this
      code learns the model and uses it immediately on the same data:</para>

      <programlisting>Model := BayesModule.LearnD(D2(Number&lt;=3),D2(Number=4));
Results := BayesModule.ClassifyD(D2(Number&lt;=3),Model);
Results;
</programlisting>

      <sect2 id="Logisitic_regression">
        <title id="Logistic_Regression">Logistic Regression</title>

        <para>Regression analysis includes techniques for modeling the
        relationship between a dependent variable Y and one or more
        independent variables Xi.</para>

        <para>The most common form of regression model is the Ordinary Linear
        Regression (OLR) which fits a line through a set of data
        points.</para>

        <para>While the linear regression model is simple and very applicable
        to many cases, it is not adequate for some purposes. For example, if
        dependent variable Y is binary, i.e. if Y takes either 0 or 1, then a
        linear model, which has no bounds on what values the dependent
        variable Y can take, cannot represent the relationship between X and Y
        correctly. In that case, the relationship can be modeled using a
        logistic function, also known as sigmoid function, which is an
        S-shaped curve with values from (0,1). Since dependent variable Y can
        take only two values, 0 or 1, the Logistic Regression model predicts
        two outcomes, 0 or 1, and it can be used as a tool for
        classification.</para>

        <para>For example, given the following data set:<informaltable>
            <tgroup cols="2">
              <colspec align="left" colwidth="80pt" />

              <colspec align="left" colwidth="80pt" />

              <thead>
                <row>
                  <entry align="left">X</entry>

                  <entry align="left">Y</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry align="left">1</entry>

                  <entry>0</entry>
                </row>

                <row>
                  <entry align="left">2</entry>

                  <entry>0</entry>
                </row>

                <row>
                  <entry align="left">3</entry>

                  <entry>0</entry>
                </row>

                <row>
                  <entry align="left">4</entry>

                  <entry>0</entry>
                </row>

                <row>
                  <entry align="left">5</entry>

                  <entry>1</entry>
                </row>

                <row>
                  <entry align="left">6</entry>

                  <entry>0</entry>
                </row>

                <row>
                  <entry align="left">7</entry>

                  <entry>1</entry>
                </row>

                <row>
                  <entry align="left">8</entry>

                  <entry>1</entry>
                </row>

                <row>
                  <entry align="left">9</entry>

                  <entry>1</entry>
                </row>

                <row>
                  <entry align="left">10</entry>

                  <entry>1</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable></para>

        <para>The Logistic Regression produces the model below:</para>

        <para><graphic fileref="images/ML021.jpg" /></para>

        <para>This model is then used as a classifier which assigns class 0 to
        every point xi where yi&lt;0.5, and class 1 for every xi where
        yi&gt;=0.5.</para>

        <para>In the following example we have a dataset equivalent to the
        dataset used to create the Logistic Regression model depicted
        above.</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

value_record := RECORD
                UNSIGNED rid;
                REAL length;
                INTEGER1 class; 
END;

d := DATASET([{1,1,0}, {2,2,0}, {3,3,0}, {4,4,0}, {5,5,1},
              {6,6,0}, {7,7,1}, {8,8,1}, {9,9,1}, {10,10,1}]
             ,value_record);

ML.ToField(d,o);
Y := O(Number=2);  // pull out class
X := O(Number=1); // pull out lenghts
                                                                                                                
dY := ML.Discretize.ByRounding(Y);  
LogisticModule := ML.Classify.Logistic(,,10);
Model := LogisticModule.LearnC(X,dY);
LogisticModule.ClassifyC(X,Model);

</programlisting>The classifier produces the following result:<graphic
        fileref="images/ML022.jpg" /></para>

        <para>As expected, the independent variable values 5 and 6 have been
        mis-classified compared to the training set, but the confidence in
        those classification results is low. As depicted in the logistic
        regression figure above, mis-classification happens because the model
        function value for x=5 is less than 0.5 and it is greater than 0.5 for
        x=6.</para>
      </sect2>
    </sect1>

    <sect1 id="Cluster_Walkthrough">
      <title>Cluster Walk-through</title>

      <para><emphasis role="bold">Modules: Cluster, Doc</emphasis></para>

      <para>The cluster module contains routines that can be used to find
      groups of records that appear to be ‘fairly similar’.</para>

      <para>The module has been shown to work on records with as few as two
      fields and as many as sixty thousand. The latter was used for clustering
      documents of words (see Using ML with documents). The clustering module
      has more than half a dozen different ways of measuring the distance
      (defining ‘similar’) between two records but it is also possible to
      write your own.</para>

      <para>Below are walk-throughs for the methods covered in the ML.Cluster
      module. Each begins with the following set of entities in 2-dimensional
      space, where the values on each axis are restricted to between 0.0 and
      10.0:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
lMatrix:={UNSIGNED id;REAL x;REAL y;};

dEntityMatrix:=DATASET([
  {1,2.4639,7.8579},
  {2,0.5573,9.4681},
  {3,4.6054,8.4723},
  {4,1.24,7.3835},
  {5,7.8253,4.8205},
  {6,3.0965,3.4085},
  {7,8.8631,1.4446},
  {8,5.8085,9.1887},
  {9,1.3813,0.515},
  {10,2.7123,9.2429},
  {11,6.786,4.9368},
  {12,9.0227,5.8075},
  {13,8.55,0.074},
  {14,1.7074,3.9685},
  {15,5.7943,3.4692},
  {16,8.3931,8.5849},
  {17,4.7333,5.3947},
  {18,1.069,3.2497},
  {19,9.3669,7.7855},
  {20,2.3341,8.5196}
],lMatrix);
ML.ToField(dEntityMatrix,dEntities);
</programlisting></para>

      <para><emphasis role="bold">Note:</emphasis> The use of the ToField
      macro which converts the original rectangular matrix, dEntityMatrix,
      into a table in the standard NumericField format that is used by the ML
      library named “dEntities”.</para>

      <sect2 id="kmeans">
        <title>KMeans</title>

        <para>With k-means clustering the user creates a second set of
        entities called centroids, with coordinates in the same space as the
        entities being clustered. The user defines the number of centroids (k)
        to create, which will remain constant during the process and therefore
        represents the number of clusters that will be determined. For our
        example, we will define four centroids:</para>

        <para><programlisting><?dbfo keep-together="always"?>dCentroidMatrix:=DATASET([
  {1,1,1},
  {2,2,2},
  {3,3,3},
  {4,4,4}
],lMatrix);

ML.ToField(dCentroidMatrix,dCentroids);
</programlisting>As with the entity matrix, we have used ToField to convert
        the centroid matrix into the table “dCentroids”.</para>

        <para><emphasis role="bold">Note:</emphasis> Although these points are
        arbitrary, they are clearly not random.</para>

        <para>These points form an asymmetrical pattern in one corner of the
        space. This is to highlight a feature of k-means clustering which is
        that the centroids will end up in the same resting place (or very
        close to it) regardless of where they started. The only caveat related
        to centroid positioning is that no two centroids should occupy the
        same initial location.</para>

        <para>Now that we have our centroids, they are now subjected to a
        2-step iterative re-location process. For each iteration we determine
        which entities are closest to which centroids, then we recalculate the
        position of the centroids based as the mean location of all of the
        entities affiliated with them.</para>

        <para>To set up this process, we make the following call to the KMeans
        routine:</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans:=ML.Cluster.KMeans(dEntities,dCentroids,30,.3);
</programlisting>Here, we are passing in our two datasets, dEntities and
        dCentroids. In order to prevent infinite loops, we also must specify a
        maximum number of iterations, which is set to 30 in the above
        example.</para>

        <para>Convergence is defined as the point at which we can say the
        centroids have found their final resting places. Ideally, this will be
        when they stop moving completely.</para>

        <para>However, there will be situations where centroids may experience
        a “see-sawing” action, constantly trading affiliations back and forth
        indefinitely. To address this, we have the option of specifying a
        positive value as the convergence threshold. The process will assume
        convergence if, during any iteration, no centroid moves a distance
        greater than that number. In our above example, we are setting the
        convergence threshold to 0.3. If no threshold is specified, then the
        threshold is set to 0.0. If the process hits the maximum number of
        iterations passed in as parameter 3, then it stops regardless of
        whether convergence is achieved or not.</para>

        <para>The final parameter, which is also optional, specifies which
        distance formula to use. For our example we are leaving this parameter
        blank, so it defaults to a simple Euclidean calculation, but we could
        easily change this by adding the fifth parameter with a value such as
        “ML.Cluster.DF.Tanimoto” or “ML.Cluster.DF.Manhattan”.</para>

        <para>Below are calls to the available attributes within the KMeans
        module:</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.AllResults;
</programlisting>This will produce a table with a layout similar to
        NumericField, but instead of a single value field, we have a field
        named “values” which is a set of values.</para>

        <para>Each row will have the same number of values in this set, which
        is equal to the number of iterations + 1. Values[1] is the initial
        value for the id/number combination, Values[2] is after the first
        iteration, etc.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.Convergence;</programlisting>Convergence
        will respond with the number of iterations that were performed, which
        will be an integer between 1 and the maximum specified in the
        parameters. If it is equal to the maximum, then you may want to
        increase that number or specify a higher convergence threshold because
        it had not yet achieved convergence when it completed.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.Result();  // The final locations of the centroids
MyKMeans.Result(3); // The results of iteration 3
</programlisting>Results will respond with the centroid locations after the
        specified number of iterations. If no number is passed, this will be
        the locations after the final iteration.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.Delta(3,5); // The distance every centroid travelled across each axis from 
                        iterations 3 to 5
MyKMeans.Delta(0);   // The total distance the centroids travelled on each axis from the 
                        beginning to the end
</programlisting>Delta displays the distance traveled <emphasis>on each
        axis</emphasis> between the iterations specified in the parameters. If
        no parameters are passed, this will be the delta between the last two
        iterations.</para>

        <para><programlisting><?dbfo keep-together="always"?>MyKMeans.DistanceDelta(3,5); // The straight-line distance travelled by each centroid from 
                                iterations 3 to 5
MyKMeans.DistanceDelta(0);   // The total straight-line distance each centroid travelled 
MyKMeans.DistanceDelta();    // The distance traveled by each centroid during the last 
                                iteration.
</programlisting>DistanceDelta is the same as Delta, but displays the DISTANCE
        delta as calculated using whichever method the KMeans routine was
        instructed to use, which in our example is Euclidean.</para>

        <para>The function Allegiances provides a table of all the entities
        and the centroids to which they are closest, along with the actual
        distance between them. If a parameter is passed, it is the iteration
        number after which to sample the allegiances. If no parameter is
        passed, the convergence iteration is assumed.</para>

        <para>A second function, Allegiance, enables the user to pinpoint a
        specific entity to determine its allegiance. This function requires
        one parameter, which is the ID of the entity to poll. The second
        parameter is the iteration number and has the same behavior as with
        Allegiances. The return value for Allegiance is an integer
        representing the value of the centroid to which the entity is
        allied.</para>

        <programlisting>MyKMeans.Allegiances();     // The table of allegiances after convergence
MyKMeans.Allegiance(10,5);  // The centroid to which entity #10 is closest after iteration 5
</programlisting>
      </sect2>

      <sect2 id="AggloN">
        <title>AggloN</title>

        <para>With Agglomerative, or Hierarchical, clustering there is no need
        for a centroid set. This method takes a bottom-up approach whereby it
        identifies those pairs that are mutually closest and marries them so
        they are treated as a single entity during the next iteration. Allowed
        to run until full convergence, every entity will eventually be
        stitched up into a single tree structure with each fork representing
        tighter and tighter clusters.</para>

        <para>We set up this clustering routine using the following
        call:</para>

        <para><programlisting><?dbfo keep-together="always"?>MyAggloN:=ML.Cluster.AggloN(dEntities,4);</programlisting>Here,
        we are passing in our sample data set and telling the routine that we
        want a maximum of 4 iterations.</para>

        <para>There are two further parameters that the user may pass, both of
        which are optional. Parameter 3 enables the user to specify the
        distance formula exactly as we could in Parameter 5 of the KMeans
        routine. And as with our KMeans example, we will leave this blank so
        it defaults to Euclidean.</para>

        <para>Parameter 4 enables us to specify how we want to represent
        distances where clustered entities are involved. After the first
        iteration some of the entities will have been grouped together and we
        need to make a decision about how we measure distance to those groups.
        The three options are min_dist, max_dist, and ave_dist, which will
        instruct the routine to use the minimum distance within the cluster,
        the maximum or the average respectively. The default, which we are
        accepting for this example, is min_dist.</para>

        <para>The following three calls will give us the results of the
        Agglomerative clustering call in different ways:</para>

        <para><programlisting>MyAggloN.Dendrogram;</programlisting>The
        Dendrogram call displays the output as a string representation of the
        tree diagram. Clusters are grouped within curly braces ({}), and
        clusters of clusters are grouped in the same manner. The ID for each
        cluster will be assigned the lowest ID of the entities it encompasses.
        In our example, we end up with five clusters, and two entities yet to
        be clustered. This is because we specified a maximum of four
        iterations which was not enough to group everything together.</para>

        <para><programlisting>MyAggloN.Distances;</programlisting>The
        Distances output displays all of the remaining distances that would be
        used to further cluster the entities. If we had achieved convergence,
        this would be an empty table and our Dendrogram output would be a
        single line with every item found within the tree string. But since we
        stopped iterating early, we still have items to cluster, and therefore
        still have distances to display. The number of rows here will be equal
        to n*n-1, where n is the number of rows in the Dendrogram
        table.</para>

        <para><programlisting>MyAggloN.Clusters;</programlisting>Clusters will
        display each entity, and the ID of the cluster that the entity was
        assigned to. In our example, every entity will be assigned to one of
        the seven cluster IDs found in the Dendrogram. If we had allowed the
        process to continue to convergence, which for our sample set is
        achieved after 9 iterations, every entity will be assigned the same
        cluster ID because it will be the only one left in the
        Dendrogram.</para>
      </sect2>
    </sect1>

    <sect1>
      <title id="Correlations_Walkthrough">Correlations Walk-through</title>

      <para>Most of the algorithms within the ML libraries assume that one of
      your inputs are a collection features of a particular object and the
      algorithm exists to predict some other feature based upon the features
      you have. In the literature the ‘features you have’ are usually referred
      to as the ‘independent variables’ and the features you are trying to
      predict are called the ‘dependent variables’.</para>

      <para>Masked within those names is an assumption that is almost never
      true. That the features you have for a given object are actually
      independent of each other. Consider, for example, a classification
      algorithm that tries to predict risk of heart disease based upon height,
      weight, age and gender. The independent variables are not even close to
      being independent. Pick any two of those variables and there is a known
      link between them (even age and gender; women live longer). These
      linkages between the ‘independent’ variables usually represent an error
      factor in the algorithm used to compute the dependent variable.</para>

      <para>The Correlation module exists to allow you to quantify the degree
      of relatedness between a set of variables. There are three measures
      provided. Under the title ‘simple’ the Covariance and Pearson statistic
      is provided for every pair of variables. The Kendall measure provides
      the Kendal Tau statistic for every pair of variables; it should be noted
      that computation of Kendall’s Tau is an O(N^2) process. This will hurt
      on very large datasets.</para>

      <para>The definition and interpretation of these terms can be found in
      any statistical text; for example: <ulink
      url="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient</ulink>
      <ulink
      url="http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient">http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient</ulink></para>

      <para>(we implement tau-a)</para>

      <programlisting>import ml;

value_record := RECORD
                unsigned rid;
  real height;
                real weight;
                real age;
                integer1 species;
                integer1 gender; // 0 = unknown, 1 = male, 2 = female
  END;

d := dataset([{1,5*12+7,156*16,43,1,1},
              {2,5*12+7,128*16,31,1,2},
              {3,5*12+9,135*16,15,1,1},
              {4,5*12+7,145*16,14,1,1},
              {5,5*12-2,80*16,9,1,1},
              {6,4*12+8,72*16,8,1,1},
              {7,8,32,2.5,2,2},
              {8,6.5,28,2,2,2},
              {9,6.5,28,2,2,2},
              {10,6.5,21,2,2,1},
              {11,4,15,1,2,0},
              {12,3,10.5,1,2,0},
              {13,2.5,3,0.8,2,0},
              {14,1,1,0.4,2,0}
             ]
             ,value_record);
// Turn into regular NumericField file (with continuous variables)
ml.ToField(d,o);

Cor := ML.Correlate(o);
Cor.Simple;
Cor.Kendall;
</programlisting>
    </sect1>

    <sect1 id="Discretize_Walkthrough">
      <title>Discretize Walk-through</title>

      <para><emphasis role="bold">Modules: Discretize</emphasis></para>

      <para>As discussed briefly in the section on data models, it is not
      unusual for data to be provided in a manner where the data forms some
      real value. For example a height or weight might be measured down to the
      inch or ounce, or a price might be measured down to the nearest cent.
      Yet in terms of predictiveness we might expect
      <emphasis>similar</emphasis> values in those fields to exhibit similar
      behavior in some particular regard.</para>

      <para>Some of the ML modules expect the input data to have been
      <emphasis>banded</emphasis>, or for data which was originally in
      <emphasis>real values</emphasis> to have been turn into a set of
      discrete bands. More concretely, they require data in the DiscreteField
      format even if it was originally provided in NumericField format.</para>

      <para>The Discretize module exists to perform this conversion. All of
      the examples in this walk-through use the first dataset (d) from the
      NumericField walk-through. It might help you to just quickly look at
      that dataset again in both original and NumericField form to remind you
      of the format.</para>

      <para><programlisting><?dbfo keep-together="always"?>ml.ToField(d,o);
d;
o;
</programlisting>There are currently 3 main methods available to create
      discrete values, ByRounding, ByBucketing and ByTiling.</para>

      <para>All three methods operate upon all the data handed to them.
      Applying different methods to different fields is very easy using the
      methods discussed in the section on Data Models. This simple example
      Auto-buckets columns 2 &amp; 3 into four bands, tiles column 1 into 6
      bands and the rounds the fourth column to the nearest integer:</para>

      <para><programlisting>disc := ML.Discretize.ByBucketing(o(Number IN [2,3]),4)+ML.Discretize.ByTiling(o(Number IN 
                                              [1]),6)+ML.Discretize.ByRounding(o(Number=4));
disc;
</programlisting>It may be observed that in the description above I was able
      to describe how to discretize data a number of different ways but could
      not give any firm guidelines as to the exactly number of bands or the
      exact best method to use for any given item of data. That is because
      whilst there are schemes and guidelines out there, there is no firm
      consensus as to which are the best. It is quite possible that the best
      way to work is to ‘try a few’ and see which gives you the best
      results!</para>

      <para>The Discretize module supports this ‘suck it and see’ approach by
      allowing you to specify the discretization methods entirely within data.
      The core of this is the ‘Do’ command that takes a series of instructions
      and discretizes a dataset based upon those instructions. Each
      instruction is actually a little record of type r_Method defined in the
      Discretize module and you can construct these records yourself if you
      wish. It would be fairly easy to even create a little
      <emphasis>discreting programming language</emphasis>’ and have it
      execute. For the slightly less ambitious there are a collection of
      parameterized functions that will construct an r_Method record for each
      of the three main discretize types.</para>

      <para>The following example is exactly equivalent to the
      previous:</para>

      <para><programlisting><?dbfo keep-together="always"?>// Build up the instructions into a single data file (‘mini program’)

inst := ML.Discretize.i_ByBucketing([2,3],4)+ML.Discretize.i_ByTiling([1],6)
                                            +ML.Discretize.i_ByRounding([4]);

// Execute the instructions

done := ML.Discretize.Do(o,inst);
done;

</programlisting></para>

      <sect2 id="ByRounding">
        <title>ByRounding</title>

        <para>The ByRounding method exists to convert a real number to an
        integer by ‘rounding it’. At its simplest this means that every real
        number is converted to an integer. Values that were less than a half
        go down to the nearest integer; those that were .5 or above go up.
        Therefore if you have a field that has house prices; perhaps from
        $10,000 to $1M then you potentially will end up with 990,000 different
        discrete values (ever possible dollar value).</para>

        <para>This has made the data discrete but it hasn’t really satisfied
        the problem that ‘similar values’ have an identical discrete value. We
        might expect a $299,999 dollar house to be quite similar to a $299,998
        dollar house. The ByRounding method therefore has a scale. The real
        number in the data is multiplied by the scale factor PRIOR to
        rounding.</para>

        <para>In our example, if we apply a scale of 0.0001 (1/10000) a
        $299999 house (and a $299998 house) will both get a ByRounding result
        of 30. The scale effectively reduces the range of a variable. In the
        house case a scale of 0.0001 reduces the range from “$10,000 to $1M”
        to 1-100 which is much more manageable.</para>

        <para>Sometimes the scaled ranges do not work out so neatly. Suppose
        the field is measuring height of high-school seniors. The original
        range is probably from 48 inches up to possibly 88 inches. A scale of
        0.25 is probably enough to give the number of discrete values you
        require (10), but they will range from 12 to 22 which is not
        convenient. Therefore a DELTA is available, which is ADDED to the
        value AFTER scaling but before rounding. It can therefore be used to
        bring the eventual range down to a convenient number. In this case a
        Delta of -11 would give us an eventual range of 1-11, which is
        perfect.</para>
      </sect2>

      <sect2 id="ByBucketing">
        <title>ByBucketing</title>

        <para>ByBucketing is mathematically similar to ByRounding but with
        rather more ease of use. There is a slight performance hit and rather
        less control with the ByBucketing method. Within the ByBucketing
        method you do not specify the scale or the delta, you simply specify
        the number of eventual buckets (or the number of discrete values) that
        you eventually want. It does a pre-pass of the data to compute the
        scale and delta before applying it.</para>
      </sect2>

      <sect2 id="ByTiling">
        <title>ByTiling</title>

        <para>Both of the previous methods divide the banding evenly across
        the range of the original variable. However, while the range has been
        divided evenly, the number of different records within each band could
        vary greatly. In the height example, one would expect a large number
        of children within the 60-72 inch range (rounded values of 3-7) but
        very few in bands 1 or 11.</para>

        <para>An alternative approach is not to band by some absolute range
        but rather to band on the value of a given value relative to all of
        the other records. For example, you may want to end with 10 bands
        where each band has the same number of records in it; and band 10 is
        the top 10% of the population, band nine is the second 10% etc. This
        result is achieved using the ByTiling scheme. Similar to ByBucketing
        you specify the number of bands you eventually want and the system
        will automatically allocate the field values for you.</para>

        <para><emphasis role="bold">Note</emphasis>: ByTiling does require the
        data to be sorted and so will have an NLgN performance profile.</para>
      </sect2>
    </sect1>

    <sect1 id="Docs_Walkthrough">
      <title>Docs Walk-through</title>

      <para><emphasis role="bold">Modules: Docs</emphasis></para>

      <para>The processing of textual data is a unique problem in the field of
      Machine Learning because of the highly unstructured manner in which
      humans write. There are many ways to say the same thing and many ways
      that different statements can look alike. There is an enormous body of
      research that has been performed to determine algorithms for accurately
      and efficiently extracting useful information from data such as
      electronic documents, articles, and transcriptions, all of which rely on
      human speech patterns.</para>

      <para>The Docs module of the ML library is designed to help prepare
      unstructured and semi-structured text to make it more suitable for
      further processing. This includes routines to decompose the text into
      discrete word elements and collating simple statistics on those tokens,
      such as Term Frequency and Inverse Document Frequency. Also included are
      the basic tools to help determine token association strength using
      industry-standard functions such as Support and Confidence.</para>

      <sect2 id="Tokenize">
        <title>Tokenize</title>

        <para>The Tokenize module breaks a set of raw text into its lexical
        elements. From there, it can produce a dictionary of those elements
        with weighting as well as perform integer replacement that
        significantly reduces the space overhead needed to process such large
        amounts of data.</para>

        <para>For the purposes of this walk-through we will be using the
        following limited dataset: <programlisting><?dbfo keep-together="always"?>IMPORT ML;

dSentences:=DATASET([

  {1,'David went to the market and bought milk and bread'},
  {2,'John picked up butter on his way home from work.'},
  {3,'Jill craved lemon cookies, so she grabbed some at the convenience store'},
  {4,'Mary needs milk, bread and butter to make breakfast tomorrow morning.'},
  {5,'William\'s lunch included a sandwich on wheat bread and chocolate chip cookies.'}

],ML.Docs.Types.Raw);
</programlisting></para>

        <para>The format of the initial dataset is in the Raw format in
        Docs.Types, which is a simple numeric ID and a string of free text of
        indeterminate length.</para>

        <para>It is important that a unique ID is assigned to each row so that
        we can have references not just for every word, but for every document
        as well.</para>

        <para>In the above dataset we already have assigned these IDs, but if
        your input table does not yet have them, a quick call to
        Tokenize.Enumerate will assign a sequential integer ID to the
        table:<programlisting><?dbfo keep-together="always"?>dSequenced:=ML.Docs.Tokenize.Enumerate(dSentences)</programlisting></para>

        <para>The first step in parsing the text is to run it through the
        Clean function. This is a simple function that standardizes the text
        by performing actions such as removing punctuation, converting all
        letters into capitals, and normalizing some common
        contractions.<programlisting>dCleaned:=ML.Docs.Tokenize.Clean(dSentences);</programlisting></para>

        <para>Once cleaned, the next step is to break out each word as a
        separate entity using the Split function. A word is defined
        intuitively as a series of non-white-space characters surrounded by
        white space.<programlisting>dSplit:=ML.Docs.Tokenize.Split(dCleaned);</programlisting></para>

        <para>The output produced from the Split function is a 3-column table
        in ML.Docs.Types.WordElement format, with the document ID, the ordinal
        position of the word within the text of that document, and the word
        itself.</para>

        <para>In our example, the first few rows of this table will
        be:<programlisting><?dbfo keep-together="always"?>1   1   DAVID
1   2   WENT
1   3   TO
</programlisting></para>

        <para>This opens us up a number of possibilities for processing our
        text. Most, require one further step, which is to derive some
        aggregate information of the words that appear in our corpus of
        documents. We do this using the Lexicon function:<programlisting>dLexicon:=ML.Docs.Tokenize.Lexicon(dSplit)</programlisting></para>

        <para>This function aggregates the data in our dSplit table, grouping
        on word. The resulting dataset contains one row for each word along
        with a unique ID (an integer starting at 1), a total count of the
        number of times the word occurs in the entire corpus, and the number
        of unique documents within which the word exists. The ID assigned to
        the word is inversely proportional to the word frequency, which means
        that the word that appears the most often will be assigned 1, the next
        most common will have 2, and so on.</para>

        <para>When processing very large amounts of text, there is an
        additional function ToO which can be used to reduce the amount of
        resources used during processing: <programlisting>dReplaced:=ML.Docs.Tokenize.ToO(dSplit,dLexicon);</programlisting></para>

        <para>The output from this function will have as many rows as there
        are in dSplit, but instead of seeing the words as they are in the
        text, you will see the word ID that was assigned to it in dLexicon.
        This saves a large amount of memory because the word ID is always
        4-byte integer, while the word is variable length and usually much
        larger. Since the function has access to the aggregate information
        collected by the Lexicon function, this information is also tacked
        back on to the output from ToO so that it is readily available if
        desired.</para>

        <para>From this point, we have the framework for performing numerous
        Natural Language Processing algorithms, such as keyword designation
        and extraction using the TF/IDF method, or even clustering by treating
        each word ID as a dimension in Euclidean space.</para>

        <para>Finally, the function FromO is pretty self-explanatory. This
        simply re-constitutes a table that was produced by the ToO function
        back into the WordElement format.</para>

        <para><programlisting>dReconstituted:=ML.Docs.Tokenize.FromO(dReplaced,dLexicon);</programlisting></para>
      </sect2>

      <sect2 id="CoLocation">
        <title id="Colocation">Co-Location</title>

        <para>The Docs.CoLocation module takes the textual analysis one step
        further than Tokenize. It harvests n-grams rather than just single
        words and enables the user to perform analyses on those n-grams to
        determine significance. The same dataset (dSentences) that was used in
        the walk-through of the Tokenize module above, is also used as the
        starting point for the examples shown below. As with Tokenize, the
        first step in processing the free text for Colocation is to map all of
        the words. This is done by calling the Words attribute, which also
        calls the Tokenize.Clean and Tokenize.Split functions respectively:
        <programlisting>dWords:=ML.Docs.CoLocation.Words(dSentences);</programlisting></para>

        <para>The AllNGrams attribute then harvests every n-gram, from
        unigrams up to the n defined by the user. This produces result in a
        table that contains a row for every unique id/n-gram combination. In
        the following line, we are asking for anything up to a 4-gram. If the
        n parameter is left blank, the default is 3. <programlisting>dAllNGrams:=ML.Docs.CoLocation.AllNGrams(dWords,,4);</programlisting></para>

        <para><emphasis role="bold">Note:</emphasis> The above call has left
        the second parameter blank.</para>

        <para>The second parameter is a reference to a Lexicon which is used
        if you decide to perform integer replacement on the words prior to
        processing. This is advisable for very large corpuses. In such a case,
        we would have first called the Lexicon function (which exists in
        CoLocation as a pass-through of the same function in Tokenize) and is
        then passed that output as the second parameter:<programlisting>dLexicon:=ML.Docs.CoLocation.Lexicon(dWords);
dAllNGrams:=ML.Docs.CoLocation.AllNGrams(dWords,dLexicon,4);
</programlisting></para>

        <para>Below are calls to the standard metrics that are currently built
        into the CoLocation module. Remember that the call to Words above has
        called Tokenize.Clean, which has converted all characters in the text
        to uppercase:<programlisting><?dbfo keep-together="always"?>// SUPPORT: User passes a SET OF STRING and the output from the ALLNGrams attribute
ML.Docs.CoLocation.Support(['MILK','BREAD','BUTTER'],dAllNGrams);

// CONFIDENCE, LIFT and CONVICTION: User passes in two SETS OF STRING and the AllNGrams 
   output.
// In each case, set 1 and set 2 are read as “1=&gt;2”.  Note that 1=&gt;2 DOES NOT EQUAL 2=&gt;1.
ML.Docs.CoLocation.Confidence(['MILK','BREAD'],['BUTTER'],dAllNGrams);
ML.Docs.CoLocation.Lift(['MILK','BREAD'],['BUTTER'],dAllNGrams);
ML.Docs.CoLocation.Conviction(['MILK','BREAD'],['BUTTER'],dAllNGrams);
</programlisting></para>

        <para>To further distill the data the user may call NGrams. This
        strips the document IDs and groups the table so that there is one row
        per unique n-gram. Included in this output is aggregate information
        including the number of documents in which the item appears, the
        percentage of that compared to the document count, and the Inverse
        Document Frequency (IDF).<programlisting>dNGrams:=Docs.CoLocation.NGrams(dAllNGrams);</programlisting></para>

        <para>With the output from NGrams there are other attributes that can
        be called to further analyze the data.</para>

        <para>Calling SubGrams produces a table of every n-gram where n&gt;1
        along with a comparison of the document frequency of the n-gram to the
        product of the frequencies of all of its constituent unigrams.</para>

        <para>This gives an indication of whether the phrase or its parts may
        be more significant in the context of the corpus.</para>

        <para><programlisting>ML.Docs.CoLocation.SubGrams(dNGrams);</programlisting></para>

        <para>Another measure of significance is SplitCompare. This splits
        every n-gram with n&gt;1 into two rows with two parts which are the
        initial unigram and the remainder, and the final unigram and the
        remainder. The document frequencies of all three items (the full
        n-gram, and the two constituent parts) are then presented side-by-side
        so their relative values can be evaluated. This helps to determine if
        a leading or trailing word carries any weight in the encompassing
        phrase.<programlisting>ML.Docs.CoLocation.SplitCompare(dNGrams);</programlisting></para>

        <para>Once any analysis has been done and the user has phrases of
        significance, they can be re-constituted using a call to
        ShowPhrase:<programlisting>ML.Docs.CoLocation.ShowPhrase(dLexicon,’14 13 4’);  // would return ‘CHOCOLATE CHIP COOKIES’</programlisting></para>
      </sect2>
    </sect1>

    <sect1 id="Field_aggregates_walkthrough">
      <title id="Field_Aggregates_Walkthrough">Field Aggregates
      Walkthrough</title>

      <para><emphasis role="bold">Modules: FieldAggregates,
      Distribution</emphasis></para>

      <para>The FieldAggregates module exists to provide statistics upon each
      of the fields of a file. The file is passed in to the field aggregates
      module and then various properties of those fields can be queried, for
      example:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
// Generate random data for testing purposes
TestSize := 10000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
// Pass the test data into the Aggregate Module
Agg := ML.FieldAggregates(D);
Agg.Simple; // Compute some common statistics
</programlisting>This example provides two rows. The ‘number’ column ties the
      result back to the column being passed in. There are columns for
      min-value, max-value, the sum, the number of rows (with values), the
      mean, the variance and the standard deviation. The ‘simple’ attribute is
      a very good one to use on huge data as it is a simple linear
      process.</para>

      <para>The aggregate module is also able to ‘rank order’ a set of data;
      the SimpleRanked attribute allocates every value in every field a number
      – the smallest value gets the number 1, then 2 etc. The ‘Simple’
      indicator is to denote that if a value is repeated the attribute will
      just arbitrarily pick which one gets the lower ranking.</para>

      <para>As you might expect there is also a ‘ranked’ attribute. In the
      case of multiple identical values this will assign every value with the
      same value a rank which is the average value of the ranks of the
      individual items, for example:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
TestSize := 50;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.SimpleRanked;
Agg.Ranked;
</programlisting></para>

      <para><emphasis role="bold">Note:</emphasis> Ranking requires the data
      to be sorted; therefore ranking is an ‘NlgN’ process.</para>

      <para>When examining the results of the ‘Simple’ attribute you may be
      surprised that two of the common averages ‘median’ and ‘mode’ are
      missing. While the Aggregate module can return those values, they are
      not included in the ‘Simple’ attribute because they are NLgN processes
      and we want to keep ‘Simple’ as cheap as possible. The median values for
      each column can be obtained using the following:</para>

      <para><programlisting>Agg.Medians;</programlisting>The modes are found
      by using:<programlisting>Agg.Modes;</programlisting></para>

      <para>It is possible that more than one mode will be returned for a
      particular column, if more than one value has an equal count.</para>

      <para>The final group of features provided by the Aggregate module are
      the NTiles and the Buckets. These are closely related but totally
      different which can be confusing.</para>

      <para>The NTiles are closely related to terms like ‘percentiles’,
      ‘deciles’ and ‘quartiles’, which allow you to grade each score according
      the a ‘percentile’ of the population. The name ‘N’ tile is there because
      you get to pick the number of groups the population is split into. Use
      NTile(4) for quartiles, NTile(10) for deciles and NTile(100) for
      percentiles. NTile(1000) can be used if you want to be able to split
      populations to one tenth of a percent. Every group (or Tile) will have
      the same number of records within it (unless your data has a lot of
      duplicate values because identical values land in the same tile). The
      following example demonstrates the possible use of NTiling.</para>

      <para>Imagine you have a file with people and for each person you have
      two columns (height and weight). NTile that file with a number, such as
      100. Then if the NTile of the Weight is much higher than the NTile of
      the Height, the person might be overweight. Conversely if the NTile of
      the Height is much higher than the Weight then the person might be
      underweight. If the two percentiles are the same then the person is
      ‘normal’.</para>

      <para>NTileRanges returns information about the highest and lowest value
      in every Tile. Suppose you want to answer the question: “what are the
      normal SAT scores for someone going to this college”. You can compute
      the NTileRanges(4). Then you can note both the low value of the second
      quartiles and the high value of the third quartile and declare that “the
      middle 50% of the students attending that college score between X and
      Y”.</para>

      <para>The following example demonstrates this:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
TestSize := 100;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.NTiles(4);
Agg.NTileRanges(4)
</programlisting>Buckets provide very similar looking results. However buckets
      do NOT attempt to divide the groups so that the population of each group
      is even. Buckets are divided so that the RANGE of each group is even.
      Suppose that you have a field with a MIN of 0 and MAX of 50 and you ask
      for 10 buckets, the first bucket will be 0 to (almost)5, the second 5 to
      (almost) 10 etc. The Buckets attribute assigns each field value to the
      bucket. The BucketRanges returns a table showing the range of each
      bucket and also the number of elements in that bucket. If you wanted to
      plot a histogram of value verses frequency, for example, buckets would
      be the tool to use.</para>

      <para>The final point to mention is that many of the more sophisticated
      measures use the simpler measures and also share other more complex code
      between themselves. If you eventually want two or more of these measures
      for the same data it is better to compute them all at once. The ECL
      optimizer does an excellent job of making sure code is only executed
      once however often it is used. If you are familiar with ECL at a lower
      level, you may wish to look at the graph for the following:
      <programlisting><?dbfo keep-together="always"?>IMPORT ML;
TestSize := 10000000;
a1 := ML.Distribution.Uniform(0,100,10000); 
b1 := ML.Distribution.GenData(TestSize,a1,1); // Field 1 Uniform
a2 := ML.Distribution.Poisson(3,100);
b2 := ML.Distribution.GenData(TestSize,a2,2);
D := b1+b2; // This is the test data
Agg := ML.FieldAggregates(D);
Agg.Simple;
Agg.SimpleRanked;
Agg.Ranked;
Agg.Modes;
Agg.Medians;
Agg.NTiles(4);
Agg.NTileRanges(4);
Agg.Buckets(4);
Agg.BucketRanges(4)
</programlisting></para>
    </sect1>

    <sect1>
      <title id="Matrix_Library_Walkthrough">Matrix Library
      Walk-through</title>

      <para>The Matrix Library provides a number of matrix manipulation
      routines. Some of them are standard matrix operations that do not
      require any specific explanations (Add, Det, Inv, Mul, Scale, Sub, and
      Trans). Others are a bit less standard, and have been created to provide
      the appropriate functional support for other ML library
      algorithms.</para>

      <programlisting>IMPORT ML;
IMPORT ML.Mat AS Mat;
d := dataset([{1,1,1.0},{1,2,2.0},{2,1,3.0},{2,2,4.0}],Mat.Types.Element);
d1:= Mat.Scale(d,10.0);
Mat.Add(d1,d);
Mat.Sub(d1, d );
Mat.Mul(d,d);
Mat.Trans(d);
Mat.Inv(d); 
</programlisting>

      <sect2>
        <title id="Each">Each</title>

        <para>The Each matrix module provides routines for element-wise
        matrix, in that it provides functions that operate on individual
        elements of the matrix. The following code starts with the square
        matrix whose elements are equal to 2.</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,2.0},{1,2,2.0},{1,3,2.0},
              {2,1,2.0}, {2,2,2.0},{2,3,2.0}, 
              {3,1,2.0},{3,2,2.0}, {3,3,2.0}], ML.Mat.Types.Element);
AA := ML.Mat.Each.Mul(A,A);
A_org := ML.Mat.Each.Sqrt(AA);
OneOverA := ML.Mat.Each.Reciprocal(A_org,1);
ML.Mat.Each.Mul(A_org,OneOverA);
</programlisting>

        <para>The Each.Mul routine multiplies each element of the matrix A
        with itself producing the matrix AA whose elements are equal to 4. The
        Each.Sqrt routine calculates the square root of each element producing
        the matrix A_org whose elements are equal to 2. The Each.Reciprocal
        routine calculates reciprocal value of every element of the matrix
        A_org producing the matrix OneOverA whose elements are equal to
        ½.</para>
      </sect2>

      <sect2>
        <title id="Has">Has</title>

        <para>The Has matrix module provides various matrix properties, such
        as matrix dimension or matrix density.</para>
      </sect2>

      <sect2>
        <title id="Is">Is</title>

        <para>The Is matrix module provides routines to test matrix types,
        such as whether a matrix is an identity matrix, or whether it is a
        zero matrix, or a diagonal matrix, or a symmetric matrix, or a Upper
        or Lower triangular matrix.</para>
      </sect2>

      <sect2>
        <title id="Insert_Column">Insert Column</title>

        <para>You may need to insert a new column into an existing matrix,
        e.g. regression analysis usually requires a column of 1s to be
        inserted into the feature matrix X before a regression model gets
        created. InsertColumn was created for this purpose. The following
        inserts a column of 1s as the first column into the square matrix A,
        creating the 3-by-4 matrix:</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,2.0},{1,2,3.0},{1,3,4.0},
              {2,1,2.0}, {2,2,3.0},{2,3,4.0}, 
              {3,1,2.0},{3,2,3.0}, {3,3,4.0}], ML.Mat.Types.Element);
ML.Mat.InsertColumn(A, 1, 1.0);
</programlisting>
      </sect2>

      <sect2>
        <title id="MU">MU</title>

        <para>MU is a matrix universe module. Its routines make it possible to
        include multiple matrices into the same file. These routines are
        useful when it is necessary to return more than one matrix from a
        function. For example, the QR matrix decomposition process produces 2
        matrices, Q and R, and those two matrices can be combined together
        using routines from the MU module.</para>

        <para>This sample code starts with 2 square 3-by-3 matrices, A1 and
        A2. One with all elements eqaal to 1 and the other with all elements
        equal to 2. The 2 matrices are combined into one universal matrix A1MU
        + A2MU, with id=4 identifying elements of the matrix A1 and id=7
        identifying elements of matrix A2. The last two code lines extract the
        original matrices from the universal matrix A1MU + A2MU.</para>

        <programlisting>IMPORT * FROM ML;

A1 := dataset([{1,1,1.0},{1,2,1.0},{1,3,1.0},
                         {2,1,1.0}, {2,2,1.0},{2,3,1.0}, 
                         {3,1,1.0},{3,2,1.0}, {3,3,1.0}], ML.Mat.Types.Element);

A2 := dataset([{1,1,2.0},{1,2,2.0},{1,3,2.0},
                         {2,1,2.0}, {2,2,2.0},{2,3,2.0}, 
                         {3,1,2.0},{3,2,2.0}, {3,3,2.0}], ML.Mat.Types.Element);

A1MU := ML.Mat.MU.To(A1, 4);
A2MU := ML.Mat.MU.To(A2, 7); 
A1MU+A2MU;
ML.Mat.MU.From(A1MU+A2MU, 4);
ML.Mat.MU.From(A1MU+A2MU, 7); 
</programlisting>
      </sect2>

      <sect2>
        <title id="RepMat">Repmat</title>

        <para>The Repmat function replicates a matrix, creating a large matrix
        consisting of M-by-N tiling copies of the original matrix. For
        example, the following code starts from a matrix with one element with
        value = 2. It then creates a 3x2 matrix out of it by replicating this
        single element matrix 3 times vertically, to create a 3x1 vector. This
        vector is then replicated 2 times horizontally.</para>

        <programlisting>IMPORT * FROM ML;

A := DATASET ([{1,1,2.0}], ML.Mat.Types.Element);
B := ML.Mat.Repmat(A,3,2);
</programlisting>

        <para>The resulting matrix B is a 3x2 matrix with all elements having
        a value of 2, as in:</para>

        <programlisting>DATASET([{1,1,2.0},{1,2,2.0},{2,1,2.0}, {2,2,2.0},{3,1,2.0},{3,2,2.0}], ML.Mat.Types.Element); </programlisting>

        <para>The Repmat function can be used to adjust the mean values of the
        columns of a given matrix. This can be achieved by first calculating
        the mean values of every matrix column using the mcA :=
        Has(A).MeanCol. This function generates a row vector mcA containing
        mean values for every matrix column. This row vector then needs to be
        replicated vertically to match the size of the original matrix, which
        can be achieved using the rmcA := Repmat(mcA, Has(A).Stats.XMax, 1).
        The rmcA matrix is the same size as the original matrix A, and its
        column values are the same for every matrix column and they are equal
        to the mean value of that column. Finally, if we subtract the rmcA
        from matrix A, we get a matrix whose columns have the mean value of
        zero. This can be achieved using the following compact code:</para>

        <programlisting id="Repmat">IMPORT * FROM ML;
A := dataset([{1,1,2.0},{1,2,3.0},{1,3,4.0},
              {2,1,2.0}, {2,2,3.0},{2,3,4.0}, 
              {3,1,2.0},{3,2,3.0}, {3,3,4.0}], ML.Mat.Types.Element); 
ZeroMeanA := ML.Mat.Sub(A, ML.Mat.Repmat(ML.Mat.Has(A).MeanCol, 
ML.Mat.Has(A).Stats.XMax, 1));
</programlisting>
      </sect2>

      <sect2>
        <title id="Decomp">Decomp</title>

        <para>The Decomp matrix module provides routines for different matrix
        decompositions (or matrix factorizations). Different decompositions
        are needed to implement efficient matrix algorithms for particular
        cases of problems in linear algebra.</para>
      </sect2>

      <sect2>
        <title id="LU_Decomposition">LU Decomposition</title>

        <para>The LU matrix decomposition is applicable to a square matrix A,
        and it is used to help solve a system of linear equations Ax = b. When
        solving a system of linear equations Ax = b, the matrix A can be
        decomposed via the LU decomposition, which factorizes a matrix into a
        lower triangular matrix L and an upper triangular matrix U. The
        equivalent systems L(Ux) = b and Ux = Inv(L)b are easier to solve then
        the original system of linear equations Ax = b. These equivalent
        systems of linear equations are solved by ‘forward substitution’ and
        ‘back substitution’ using the f_sub and b_sub routines available in
        the Decomp module. The LU decomposition is currently being used to
        calculate the inverted matrix.</para>

        <para>The following code demonstrates how to decompose matrix A into
        its L and U components. The L and U components are calculated first.
        To validate that this matrix decomposition is done correctly, we need
        to demonstrate that A=LU. This code does that by multiplying L and U,
        and then subtracting that result from A. The expected result is a zero
        matrix (the matrix whose size is the same as the size of the original
        matrix A with all elements being equal to 0).</para>

        <para>The problem is that the arithmetic involved in calculation of L
        and U components may create some rounding error, and as a result of
        that the A-LU matrix may not have all zero elements, as some elements
        could be positive real numbers very close to zero. For example, the
        element (4,3) of the matrix A-LU in the following example has the
        value 8.881784197001252e-16, the number that differs from 0 at its
        15th decimal point. The sample code deals with that problem by
        applying the RoundDelta function to the A-LU matrix.</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,2.0},{1,2,-1.0},{1,3,3.0},{1,4,4.0},
              {2,1,4.0}, {2,2,2.0},{2,3,1.0},{2,4,5.0}, 
              {3,1,-6.0},{3,2,-1.0}, {3,3,3.0}, {3,4,6.0},
              {4,1,-6.0},{4,2,-1.0}, {4,3,1.0}, {4,4,6.0}], ML.Mat.Types.Element);
L := ML.Mat.Decomp.LComp(ML.Mat.Decomp.LU(A));
U := Ml.Mat.Decomp.UComp(ML.Mat.Decomp.LU(A));
LU := ML.Mat.Mul(L,U);
ML.MAT.RoundDelta(ML.Mat.Sub(A,LU));
</programlisting>
      </sect2>

      <sect2>
        <title id="Cholesky_Decomposition">Cholesky Decomposition</title>

        <para>The Cholesky matrix decomposition is applicable to a square,
        symmetric, and positive real matrix A. It factorizes a matrix A into
        the product of a lower triangular matrix L, and its transpose L’. It
        is mainly used for the numerical solution of system of linear
        equations Ax = b. If A is symmetric and positive, then a system of
        linear equations Ax = b can be solved by first computing the Cholesky
        decomposition A = L*L’, then solving Ly = b for y, and finally solving
        L’x = y for x.</para>

        <para>The sample code demonstrates how to create a lower triangular
        matrix L using Cholesky decomposition. The code validates this
        decomposition by multiplying L with its transpose and ensuring that
        this product is equal to the original matrix A.</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,2.0},{1,2,1.0},{1,3,1.0},
              {2,1,1.0}, {2,2,3.0},{2,3,1.0}, 
             {3,1,1.0},{3,2,1.0}, {3,3,4.0}], ML.Mat.Types.Element);
 L := ML.Mat.Decomp.Cholesky(A);
LLt := ML.Mat.Mul(L,ML.Mat.Trans(L));
ML.MAT.RoundDelta(ML.Mat.Sub(A,LLt));
</programlisting>
      </sect2>

      <sect2>
        <title id="QR_Decomposition">QR Decomposition</title>

        <para>The QR matrix decomposition is applicable to an m-by-n matrix A.
        It factorizes matrix A into a product QR, where Q is an orthogonal
        matrix (ie Q*Q’ = I) of size m-by-m, and R is an upper triangular
        matrix of size m-by-n. The QR decomposition provides an alternative
        way of solving a system of linear equations Ax = b, without having to
        invert the matrix. Since the Q is orthonormal, the original system Ax
        = b is equivalent to Rx = Q’b = c. The system Rx = c is solved by
        ‘back substitution’, since R is triangular matrix.</para>

        <para>The following sample code demonstrates how to decompose matrix A
        into its Q and R components. The code validates this decomposition by
        demonstrating that a/ A=QR, and b/ Q is an orthonormal matrix. The
        purpose of the Thin() function is to create a sparse matrix
        representation of its input matrix, and this is achieved by removing
        all zero elements from it. Since the A-QR is expected to be a zero
        matrix, applying the Thin() function to it is expected to create an
        empty matrix. The second result is expected to be an identity matrix,
        and the Thin() function will remove all zero elements from it.</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,12.0},{2,1,6.0},{3,1,-4.0},
              {1,2,-51.0},{2,2,167.0},{3,2,24.0},
              {1,3,4.0},{2,3,-68.0},{3,3,-41.0}], ML.MAT.Types.Element);

Q := ML.Mat.Decomp.QComp(A);
R := ML.Mat.Decomp.RComp(A);
QR := ML.Mat.Mul(Q,R);
ML.Mat.Thin(ML.MAT.RoundDelta(ML.Mat.Sub(A,QR)));
ML.Mat.Thin(ML.MAT.RoundDelta(ML.Mat.Mul(Q,ML.Mat.Trans(Q))));
</programlisting>
      </sect2>

      <sect2>
        <title id="Eigen_Decomposition">Eigen Decomposition (Eigenvalues and
        Eigenvectors)</title>

        <para>Eigen decomposition is a factorization of a matrix, where the
        original matrix is represented in terms of its eigenvalues and
        eigenvectors. Indirect definitions of eigenvalues and eigenvectors can
        be stated as, "given a square matrix A, the number λ is an eigenvalue
        of A if there exists a non-zero vector v such that A*v =λ*v." If such
        a vector v exists, then v is called an eigenvector of A corresponding
        to the eigenvalue λ.</para>

        <para>In general, when matrix is multiplied with a vector, it changes
        both the vector’s magnitude and direction. However, for some vectors,
        A*v can only change the vector’s magnitude, while leaving its
        direction unchanged. These vectors are eigenvectors of the matrix. A
        matrix changes a vector’s magnitude by multiplying its magnitude by a
        factor, which is positive if the vector’s direction does not change
        and is negative if the vector’s direction is reversed. This factor is
        the eigenvalue λ associated with that eigenvector v.</para>

        <para>Here is another way of looking at eigenvectors and eigenvalues.
        An eigenvector of matrix A is a vector that maintains its direction
        after undergoing a linear transformation A (i.e. after matrix
        multiplication A*v), and an eigenvalue is the scalar value that the
        eigenvector was multiplied by during the linear transformation
        A.</para>

        <para>Eigen decomposition can be expressed as matrix factorization
        into matrices V and L which satisfy the following equation: A*V = V*L,
        where V is the square matrix whose columns are the eigenvectors of A,
        and L is a diagonal matrix whose diagonal elements are eigenvalues of
        A. The code below demonstrates how to use the Eig function to perform
        an Eigen decomposition of a matrix:</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,12.0},{2,1,6.0},{3,1,4.0},
              {1,2,6.0},{2,2,167.0},{3,2,24.0},
              {1,3,4.0},{2,3,24.0},{3,3,-41.0}], ML.MAT.Types.Element);

eig_values:=ML.Mat.Eig(A).valuesM;
eig_valuesV:=ML.Mat.Eig(A).valuesV;
eig_vectors:=ML.Mat.Eig(A).vectors;
cnt:=(INTEGER)ML.Mat.Eig(A).convergence;  
</programlisting>

        <para>The ValuesM attribute returns eigenvalues in the diagonal matrix
        format. The valueV returns those very same eigenvalues but in a vector
        format. The vectors attribute returns a matrix whose columns are
        eigenvectors corresponding to the eigenvalues returned by either
        valuesV or valuesM attributes.</para>

        <para>The eigenvectors and eigenvalues are calculated using an
        iterative process, with the maximum number of iterations set to 200.
        The convergence attribute returns the number of iteration it took to
        the Eigen decomposition process to converge. If the number returned by
        the convergence attribute equals to 200, then it is likely that the
        decomposition process needs more than 200 iterations to converge. The
        number of iterations can be increased to 500 for example, using the
        sample code below:</para>

        <programlisting>eig_module:=ML.Mat.Eig(A,500);
eig_valuesV:= eig_module.valuesV;
eig_vectors:= eig_module.vectors;
cnt:=(INTEGER) eig_module.convergence;  
</programlisting>
      </sect2>

      <sect2>
        <title id="Lanczos_Algorithm">Lanczos Algorithm</title>

        <para>Matrix factorization using the Eigen decomposition is very
        expensive operation because it is based on an iterative algorithm that
        converges linearly, while performing an expensive full QR matrix
        decomposition at every iteration step. A number of methods have been
        developed to improve convergence rate of the Eigen decomposition by
        transforming the original matrix into another format which preserves
        the eigenvalues of the original matrix, and makes the Eigen
        decomposition converge faster. The most efficient method of that kind
        is the Lanczos method. It is a technique for converting the original
        matrix into a tri-diagonal matrix having the same eigenvalues as the
        original matrix.</para>

        <para>The Lanczos method is also popular because it calculates an
        eigenvalue in every iteration cycle. This feature makes the Lanczos
        algorithm/method useful in situations where only a few matrix largest
        or smallest eigenvalues are needed.</para>

        <para>The following code demonstrates how to calculate eigenvalues and
        eigenvectors of matrix A using the Lanczos function to convert a
        matrix A into its tri-dimensional equivalent before applying the Eigen
        decomposition to it.</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,12.0},{2,1,6.0},{3,1,4.0},
              {1,2,6.0},{2,2,167.0},{3,2,24.0},
              {1,3,4.0},{2,3,24.0},{3,3,-41.0}], ML.MAT.Types.Element);

T:=ML.Mat.Lanczos(A,3).TComp;
V:=ML.Mat.Lanczos(A,3).VComp;

eigT_val:=ML.Mat.Eig(T).valuesM;
eigT_vec:=ML.Mat.Eig(T).vectors;
eigA_vec := ML.Mat.Mul(V,eigT_vec);
</programlisting>

        <para>The Lanczos function takes 2 arguments, the input matrix to
        process and the number of eigenvalues to calculate. It produces two
        results: the TComp attribute returns the tri-diagonal matrix having
        the same eigenvalues as the original matrix A, and the VComp returns
        transformation matrix which is needed for calculation of eigenvectors
        of the original matrix A.</para>

        <para>Once the tri-diagonal matrix T gets created, as a result of
        :</para>

        <programlisting>T:=ML.Mat.Lanczos(A,3).TComp;</programlisting>

        <para>The eigenvalues of A can be calculated using the following
        code:</para>

        <programlisting>eigA_val:=ML.Mat.Eig(T).valuesM; </programlisting>

        <para>Notice that since the tri-diagonal matrix T has the same
        eigenvalues as the original matrix A, eigT_vale=eigA_val.</para>

        <para>The eigenvectors of matrix A are calculated by multiplying the V
        matrix created by the Lanczos algorithm with the eigenvectors of the
        matrix T.</para>
      </sect2>

      <sect2>
        <title id="Singular_Value_Decomposition">Singular Value Decomposition
        (SVD)</title>

        <para>Singular value decomposition (SVD) is a factorization of a
        matrix into a product of three simpler matrices. Formally, the
        singular value decomposition of a matrix A is a factorization of the
        form A=USV' where U is an orthogonal matrix, i.e.U is a square matrix
        with real values whose columns and rows are orthogonal unit vectors, S
        is a rectangular diagonal matrix with nonnegative real numbers on the
        diagonal, and V' is a transpose of an orthogonal matrix V.</para>

        <para>The orthogonal matrices U and V by definition satisfy the
        following: U’U = UU’ = I, and V’V = VV’ = I; The columns of U are
        orthogonal and unit (i.e. orthonormal) eigenvectors of AA’, and the
        columns of V are orthonormal eigenvectors of A’A. The matrix S is a
        diagonal matrix containing the square roots of eigenvalues from U or V
        in descending order.</para>

        <para>The diagonal elements of S are known as the singular values of
        A, and the columns of U and V are called the left singular vectors and
        right singular vectors of A respectively.</para>

        <para>Singular value decomposition components of a matrix U, S and V
        can be multiplied together to recreate the original matrix exactly.
        However, if only a subset of rows and columns of matrices U, S, and V
        are used, then those lower-order matrices U, S, and V provide the best
        approximation of the original matrix in the least square error sense.
        Because of that, SVD can be seen as a method for transforming
        correlated variables represented by columns of the original matrix
        into a set of uncorrelated variables that better expose relationships
        that exist among the original data items. SVD can also be used as a
        method for identifying and ordering the dimensions along which data
        points exhibit the most variation.</para>

        <para>SVD has many applications. For example, SVD could be applied to
        natural language processing for latent semantic analysis (LSA). LSA
        starts with a matrix whose rows represent words, columns represent
        documents, and matrix values (elements) are counts of the word in the
        document. It then applies SVD to the input matrix, and uses a subset
        of most significant singular vectors and corresponding singular values
        to map words and documents into a new space, called ‘latent semantic
        space’, where documents are placed near each other measured by
        co-occurrence of words, even if those words never co-occurred in the
        training corpus.</para>

        <para>The LSA’s notion of term-document similarity can be applied to
        information retrieval, creating a system known as Latent Semantic
        Indexing (LSI). An LSI system calculates similarity several terms
        provided in a query have with documents by creating k-dimensional
        query vector as a sum of k-dimensional vector representations of
        individual terms, and comparing it to the k-dimensional document
        vectors.</para>

        <para>The code below demonstrates how to use SVD to decompose given
        matrix A into its U, S and V components:</para>

        <programlisting>IMPORT * FROM ML;
A := dataset([{1,1,2.0},{1,2,3.0},{1,3,4.0},
              {2,1,2.0}, {2,2,3.0},{2,3,4.0}, 
              {3,1,2.0},{3,2,3.0}, {3,3,4.0}], ML.Mat.Types.Element); 

U := ML.Mat.Svd(A).UComp;
S := ML.Mat.Svd(A).SComp;
V := ML.Mat.Svd(A).VComp;
</programlisting>
      </sect2>

      <sect2>
        <title id="Principal_Component_Analysis">Principal Component Analysis
        (PCA)</title>

        <para>Principal Component Analysis (PCA) is a mathematical procedure
        that uses an orthogonal transformation to convert a set of
        observations of possibly correlated variables into a set of values of
        linearly uncorrelated variables called ‘principal components’. The
        number of principal components is less than or equal to the number of
        original variables. This transformation is defined in such a way that
        the first principal component has the largest possible variance (that
        is, it accounts for as much of the variability in the data as
        possible), and each succeeding component in turn has the highest
        variance possible, under the constraint that it is orthogonal (meaning
        uncorrelated with) to the preceding components.</para>

        <para>PCA can be done by eigenvalue decomposition of a data covariance
        matrix or by singular value decomposition of a data matrix, after the
        matrix vectors (i.e. data features) have been mean adjusted and
        normalized (Wikipedia).</para>

        <para>Our PCA implementation is based on a SVD of a data covariance
        matrix, and it exports two attributes, Ureduce and ZComp. The Ureduce
        attribute returns a matrix consisting of an ordered set of principal
        component vectors representing the original data set. In other words,
        Ureduce is a matrix whose columns are orthogonal vectors (i.e. vectors
        independent from each other) ordered by their significance in
        representing features/patterns in the original data set. The ZComp
        attribute returns matrix which represents projection of the original
        data set into the new space defined by the Ureduce.</para>

        <para>The PCA module takes either one or two arguments. When only one
        argument is used, it represents the original data set in matrix
        format, where matrix columns represent individual data features. In
        that case, the Ureduce attribute returns a matrix containing all
        principal component vectors associated with the input data set. It is
        also possible to ignore the components of lesser significance by
        providing the second argument to the PCA module which represents the
        number of principal components to preserve.</para>

        <para>The following code demonstrates how to use PCA:</para>

        <programlisting>IMPORT * FROM ML;
Rec := RECORD
       UNSIGNED rid;
       REAL X1;  // x coordinates
       REAL X2;  // y coordinates
 END;
points := DATASET([{1,2.5,2.4},{2,0.5,0.7},{3,2.2,2.9},{4,1.9,2.2},{5,3.1,3.0},
                   {6,2.3,2.7},{7,2.0,1.6},{8,1.0,1.1},{9,1.5,1.6},{10,1.1,0.9}],Rec);
// Turn into regular NumericField file (with continuous variables)
ToField(points,O);
X := Types.ToMatrix(O(Number in [1,2])); 
Ur := Mat.Pca(X).Ureduce;
z := Mat.PCA(X).ZComp;
</programlisting>

        <para>The dataset points represent 10 points as a two-dimensional
        space (see the Points dataset chart below). This dataset is first
        converted into its matrix representation X, and then the principal
        component vectors Ur, and the matrix z as a projection of the matrix X
        into the Ur space are calculated.</para>

        <graphic fileref="images/ML010.jpg" />

        <para>Notice how this vector follows directions of the data pattern,
        and if we overlaid it onto data points it would go through the middle
        of data points as if it was the best fit line.</para>

        <para>The plot of the second principal component vector looks like
        this:</para>

        <graphic fileref="images/ML011.jpg" />

        <para>This second vector is orthogonal to the first one, and it
        identifies the following, less important, pattern in the data: while
        all the points follow the main line, they are off to the sides of the
        main line by some amount. If we project the original data to the Ur
        principal component vectors, we get the following results:</para>

        <informaltable>
          <tgroup cols="2">
            <colspec align="center" colwidth="100pt" />

            <colspec align="center" colwidth="100pt" />

            <thead>
              <row>
                <entry align="center">X</entry>

                <entry align="center">Y</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>0.82797</entry>

                <entry>-1.22382</entry>
              </row>

              <row>
                <entry>-1.77758</entry>

                <entry>0.142857</entry>
              </row>

              <row>
                <entry>0.992197</entry>

                <entry>0.384375</entry>
              </row>

              <row>
                <entry>0.27421</entry>

                <entry>0.130417</entry>
              </row>

              <row>
                <entry>1.675801</entry>

                <entry>-0.2095</entry>
              </row>

              <row>
                <entry>0.912949</entry>

                <entry>0.175282</entry>
              </row>

              <row>
                <entry>-0.09911</entry>

                <entry>-0.34982</entry>
              </row>

              <row>
                <entry>-1.14457</entry>

                <entry>0.046417</entry>
              </row>

              <row>
                <entry>-0.43805</entry>

                <entry>0.017765</entry>
              </row>

              <row>
                <entry>-1.22382</entry>

                <entry>-0.16268</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>

        <para>These results are shown on data plot chart below. The result of
        this data projection is equivalent to rotating the principal component
        vector 1 to make it become horizontal x-axis.</para>

        <graphic fileref="images/ML012.jpg" />

        <para>In the example above we decided to use both principal component
        vectors to transform the original data into the new space, and as a
        result, we ended up with points in the two-dimensional space with most
        of data variability along the horizontal x-axis We could also choose
        to use only the dominant component vector.</para>

        <para>We could achieve that by modifying the original code as
        follows:</para>

        <programlisting>Ur := Mat.Pca(X, 1).Ureduce; z := Mat.PCA(X,1).ZComp; </programlisting>

        <para>Where the second argument ‘1’ indicates that only one (the
        first) principal component vector is to be used for mapping of the
        original data set into the new space.</para>

        <para>If we project the original data to the first principal component
        vector, we get the following data:</para>

        <para><informaltable>
            <tgroup cols="1">
              <colspec align="center" colwidth="100pt" />

              <thead>
                <row>
                  <entry align="center">X</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>0.82797</entry>
                </row>

                <row>
                  <entry>-1.77758</entry>
                </row>

                <row>
                  <entry>0.992197</entry>
                </row>

                <row>
                  <entry>0.27421</entry>
                </row>

                <row>
                  <entry>1.675801</entry>
                </row>

                <row>
                  <entry>0.912949</entry>
                </row>

                <row>
                  <entry>-0.09911</entry>
                </row>

                <row>
                  <entry>-1.14457</entry>
                </row>

                <row>
                  <entry>-0.43805</entry>
                </row>

                <row>
                  <entry>-1.22382</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>And the following data plot is produced:</para>

        <graphic fileref="images/ML013.jpg" />

        <para>This transformation preserves only the dominant data features,
        and we can see that the second dimension of the data was
        removed.</para>
      </sect2>
    </sect1>

    <sect1 id="Regression_walkthrough">
      <title id="Regression_Walkthrough">Regression Walk-through</title>

      <para><emphasis role="bold">Modules: Regression</emphasis></para>

      <para>The Regression module exists to perform analysis and modeling of
      the relationship between a single dependent variable Y, and one or more
      independent variables Xi (also called predictor or explanatory
      variables). Regression is called ‘Simple’ if only one independent
      variable X is used. It is called ‘Multivariate’ regression when more
      than one independent variable is used.</para>

      <para>The relationship between dependent variable and independent
      variables is expressed as a function whose form has to be specified.
      Regression is called ‘Linear Regression’ if the function that defines
      the relationship is linear. For example, a Simple Linear Regression
      model expresses relationship between dependent variable Y and single
      independent variable X as a linear function:</para>

      <para>Y = β0 + β1X</para>

      <para>This function represents a line with parameters β= (β0,
      β1).</para>

      <sect2 id="Polynomial">
        <title>Polynomial</title>

        <para>The Polynomial regression expresses (ie models) relationship
        between dependent variable Y and a single independent variable X with
        polynomial function of the following form:</para>

        <para>Y = β0 + β1*LogX+ β2*X+ β3*X*LogX+ β4*X2+ β5*X2*LogX+
        β6*X3</para>

        <para>with parameters:</para>

        <para>β= (β0, β1, β2, β3, β4, β5, β6)</para>

        <para>Along with the other ML functions, Polynomial Regression is
        designed to work upon huge datasets; however it can be quite useful
        even on tiny ones. The following dataset captures the time taken for a
        particular ML routine to execute against a particular number of
        records. Polynomial Regression models relationship between the number
        of records (X=Recs), and the execution time (Y=Time) and produces β
        values, which represent how much every component of the polynomial
        function, (constant, Log, XLogX, …) contributes to the execution
        time.</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
R := RECORD
       INTEGER        rid;
       INTEGER        Recs;
       REAL           Time;
END;
d := DATASET([{1,50000,1.00},{2,500000,2.29}, {3,5000000,16.15},
               {4,25000000,80.2},{5,50000000,163},{6,100000000,316},
               {7,10,0.83},{8,1500000,5.63}],R);
ML.ToField(d,flds);
P := ML.Regress_Poly_X(flds(number=1),flds(number=2));
P.Beta;
P.RSquared
</programlisting>The Polynomial regression is configured by default to
        calculate parameters β= (β0, β1, β2, β3, β4, β5, β6). This can be
        changed to any number smaller than 6.</para>

        <para>For example, to configure polynomial regression to use the
        polynomial function Y = β0 + β1*LogX+ β2*X to model relationship
        between X and Y, the code above would need to be changed as
        follows:</para>

        <para><programlisting>P := ML.Regress_Poly_X(flds(number=1),flds(number=2), 2); </programlisting></para>

        <para>The third parameter ‘2’ has to be used to override the default
        value ‘6’.</para>
      </sect2>

      <sect2 id="OLS">
        <title>OLS</title>

        <para>The Ordinary Least Squares (OLS) regression is a linear
        regression model that calculates parameters β using the method of
        least squares to minimize the distance between measured and predicted
        values of the dependent variable Y. The following example,
        demonstrates how it might be used:</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

value_record := RECORD
                unsigned    rid;
                unsigned    age;
                real        height;
  END;

d := DATASET([{1,18,76.1}, {2,19,77}, {3,20,78.1},
            {4,21,78.2}, {5,22,78.8}, {6,23,79.7},
            {7,24,79.9}, {8,25,81.1}, {9,26,81.2},
            {10,27,81.8},{11,28,82.8}, {12,29,83.5}]
            ,value_record);

ML.ToField(d,o);

X := O(Number =1); // Pull out the age
Y := O(Number =2); // Pull out the height
Reg := ML.Regression.Sparse.OLS_LU(X,Y);
B := Reg.Beta();
B;                         
Reg.RSquared;
Reg.Anova;
</programlisting>In this example we have a dataset that contains 12 records
        with the age in months and a mean height in centimeters for children
        of that age. We use the OLS regression to find parameters β of the
        linear function, ie the line that represents (models) the relationship
        between child age and height. Once we get parameters β, we can use
        them to predict the height of a child whose age is not listed in the
        dataset.</para>

        <para>For example, the mean height of a 30 month old child can be
        predicted using the following formula:</para>

        <para>Height = β0 + 30* β1.</para>

        <para>How well does this function (ie regression model) fit the data?
        One measure of goodness of fit is the R-squared. The range of
        R-squared is [0,1], and values closer to 1 indicate better fit. For
        Simple Linear regression the R-squared represents a square of
        correlation between X and Y.</para>

        <para>Analysis of Variance (ANOVA) provides information about the
        level of variability within a regression model, and that information
        can be used as a basis for tests of significance. Anova returns a
        table with the following values for the model, error and total, and
        the model statistic called “F”:</para>

        <itemizedlist>
          <listitem>
            <para>sum of squares (SS)</para>
          </listitem>

          <listitem>
            <para>degrees of freedom (DF)</para>
          </listitem>

          <listitem>
            <para>mean square (MS)</para>
          </listitem>
        </itemizedlist>

        <para>The OLS regression can be configured to calculate parameters β
        using either the LU matrix decomposition or Cholesky matrix
        decomposition.</para>
      </sect2>
    </sect1>

    <sect1>
      <title id="Visualization_Walkthrough">Visualization Walk-through</title>

      <para>The following is a walk-through of the code located in the file
      VL.Samples. The data used in the code is drawn from the United Nations
      Statistics Division:</para>

      <programlisting>lRegions:=RECORD
  STRING continent;
  STRING region;
  UNSIGNED surface_area;
  DECIMAL8_1 pop1950;
  DECIMAL8_1 pop1960;
  DECIMAL8_1 pop1970;
  DECIMAL8_1 pop1980;
  DECIMAL8_1 pop1990;
  DECIMAL8_1 pop2000;
  DECIMAL8_1 pop2010;
END;

dRegions:=DATASET([
  {'Africa','Eastern Africa',6361,64.8,81.9,107.6,143.6,192.8,251.6,324},
  {'Africa','Middle Africa',6613,26.1,32,40.7,53.4,71.7,96.2,126.7},
  {'Africa','Northern Africa',8525,53,67.5,86.9,113.1,146.2,176.2,209.5},
  {'Africa','Southern Africa',2675,15.6,19.7,25.5,33,42.1,51.4,57.8},
  {'Africa','Western Africa',6138,70.5,85.6,107.4,139.8,182.5,235.7,304.3},
  {'Latin America','Caribbean',234,17.1,20.7,25.3,29.7,34.2,38.4,41.6},
  {'Latin America','Central America',2480,37.9,51.7,69.6,91.8,113.2,135.6,155.9},
  {'Latin America','South America',17832,112.4,147.7,191.5,240.9,295.6,347.4,392.6},
  {'North America','North America',21776,171.6,204.3,231.3,254.5,281.2,313.3,344.5},
  {'Asia','Eastern Asia',11763,672.4,801.5,984.1,1178.6,1359.1,1495.3,1574},
  {'Asia','South Central Asia',10791,507.1,620,778.8,986,1246.4,1515.6,1764.9},
  {'Asia','South Eastern Asia',4495,172.9,219.3,285.2,359,445.4,523.8,593.4},
  {'Asia','Western Asia',4831,51,66.8,86.9,114,148.6,184.4,232},
  {'Europe','Eastern Europe',18814,220.1,252.8,276.2,294.9,310.5,304.2,294.8},
  {'Europe','Northern Europe',1810,78,81.9,87.4,89.9,92.1,94.3,99.2},
  {'Europe','Southern Europe',1317,108.3,117.4,126.8,137.7,142.4,145.1,115.2},
  {'Europe','Western Europe',1108,140.8,151.8,165.5,170.4,175.4,183.1,189.1},
  {'Oceania','Australia and New Zealand',8012,10.1,12.7,15.5,17.9,20.5,23,26.6},
  {'Oceania','Melanesia',541,2.2,2.6,3.3,4.3,5.5,7,8.7},
  {'Oceania','Micronesia',3,.1,.2,.2,.3,.4,.5,.5},
  {'Oceania','Polynesia',8,.2,.3,.4,.5,.5,.6,.7}
],lRegions);
</programlisting>

      <para>In our first set of charts, we are going to slim this dataset down
      to the continent level and report on the surface area for each. This is
      accomplished with a simple TABLE command. Once we have the data we are
      looking for, we can translate it into the VL.Types.ChartData format
      using the FormatData macro:</para>

      <programlisting>dContinentArea:=TABLE(dRegions,{
  continent;
  UNSIGNED surface_area:=SUM(GROUP,surface_area);
},continent);
dContinentChart:=VL.FormatData(dContinentArea,continent);
</programlisting>

      <para>We will use all the default styles, except that we will add a
      meaningful title:</para>

      <programlisting>ContinentStyle:=VL.Styles.SetValue(Vl.Styles.Default,title,'Surface Area by Continent');</programlisting>

      <para>Once we have these components, we have all we need to produce some
      very quick and easy charts, such as the Pie, Bar and Column shown
      below:</para>

      <programlisting>VL.Chart('SurfaceAreaPie',dContinentChart,ContinentStyle).Pie;
VL.Chart('SurfaceAreaBar',dContinentChart,ContinentStyle).Bar;
VL.Chart('SurfaceAreaColumn',dContinentChart,ContinentStyle).Column;
</programlisting>

      <para>In a slightly more advanced example, we will now present
      time-oriented population statistics. As with the above example, the
      first step is to re-shape the data into just the fields we want, by
      removing the extraneous continent and surface_area fields. Then we can
      transform the result into our ChartData format.</para>

      <programlisting>dPopOnly:=PROJECT(dRegions,{lRegions AND NOT [continent,surface_area]}); 
dPopData:=VL.FormatData(dPopOnly,region);</programlisting>

      <para>At this point, if we were to paint the charts we would have the
      data points on the wrong axis and the visualization would be relatively
      meaningless for our purposes. So we run the formatted dataset through
      the function “SwapAxes”, which (as the name implies) swaps the X and Y
      axes. Since the X-Axis was originally a series of columns, the user
      needs to name the new X-Axis, which is the purpose of the second
      parameter:</para>

      <programlisting>dSwapped:=VL.SwapAxes(dPopData,'Decade'); </programlisting>

      <para>Now we may continue producing meaningful charts, such as the line
      and area charts below. A call to the SetValue Styles function like in
      the above example adds in a title as before:</para>

      <programlisting>PopStyle:=VL.Styles.SetValue(Vl.Styles.Default,title,'Population by Region over Time'); 
VL.Chart('PopulationByRegionLine',dSwapped,PopStyle).Line;
VL.Chart('PopulationByRegionArea',dSwapped,PopStyle).Area;
</programlisting>

      <para>One slightly more advanced chart the one can use from Google’s
      arsenal is the Combo chart. This one enables the user to specify two
      different types of charts that overlay each other. In the below example,
      we will plot a bar graph of the 2010 population for the four regions of
      Africa, and include a line graph showing the overall average of that
      population across all countries. In this case, we are going to filter
      our dataset on Africa, and then massage our data in the same way as
      above to remove unwanted fields.</para>

      <para>In addition to this subset, we also need to add in a row
      containing the average, which we do in the row directly following our
      filter.</para>

      <programlisting>dAfrica:=PROJECT(dRegions(continent='Africa'),{lRegions AND NOT [continent,surface_area]});
dWithAverages:=dAfrica+TABLE(dAfrica,{
  STRING region:='Average';
  DECIMAL8_1 pop1950:=AVE(GROUP,pop1950);
  DECIMAL8_1 pop1960:=AVE(GROUP,pop1960);
  DECIMAL8_1 pop1970:=AVE(GROUP,pop1970);
  DECIMAL8_1 pop1980:=AVE(GROUP,pop1980);
  DECIMAL8_1 pop1990:=AVE(GROUP,pop1990);
  DECIMAL8_1 pop2000:=AVE(GROUP,pop2000);
  DECIMAL8_1 pop2010:=AVE(GROUP,pop2010);
});
</programlisting>

      <para>Once we have built our 5-row table, we can now perform the
      SwapAxes transform. However, we still have one more thing to do that is
      a little more difficult. Producing a combo Chart requires that
      parameters be sent to Google that are not standard enough to be part of
      our default set of style parameters. As such, we need to make use of the
      ChartAdvanced attribute of the Styles module. This field exists so that
      the user may pass through specific exact strings as parameters.</para>

      <para>Such a task requires that the user have specific knowledge of the
      API they are calling, but with that knowledge the user has complete
      control over all of the buttons and knobs that are available in any of
      the API’s. In the SetValue call below, we are setting the title to
      something meaningful, we are defaulting the chart to a Bar type, and
      then changing the fifth series from Bar to Line. If the user desires
      more detail on this type of control, it is a good idea to go to the web
      site of the API and read through the documentation regarding what
      options are available.</para>

      <programlisting>dFormatted:=VL.SwapAxes(VL.FormatData(dWithAverages,region),'Decade');
ComboStyle:=Vl.Styles.SetValue(Vl.Styles.Default,ChartAdvanced,'title:"Population Growth in 
Africa",seriesType:"bars",series:{5:{type:"line"}}');
VL.Chart('AfricanPopulationGrowthCombo',dFormatted,ComboStyle).Combo;
</programlisting>
    </sect1>
  </chapter>

  <chapter id="ML_modules">
    <title id="ML_Modules">The ML Modules</title>

    <para>Each ML module focuses on a specific type of algorithm and contains
    a number of routines. The functionality of each routine is also
    described.</para>

    <para>Performance statistics are provided for some routines. These were
    carried out on a 10 node cluster and are for comparison purposes
    only.</para>

    <sect1 id="Associations">
      <title id="Associations_Module">Associations (ML.Associate)</title>

      <para>Use this module to perform frequent pattern matching on the
      underlying data. The following routines are provided:</para>

      <informaltable>
        <tgroup cols="2">
          <colspec align="left" colwidth="170pt" />

          <colspec align="left" colwidth="430pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Apriori1,Apriori2,Apriori3</entry>

              <entry>Uses ‘old school’ brute force and speed approach to
              produce patterns of up to 3 items which appear together with a
              particular degree of support.</entry>
            </row>

            <row>
              <entry>AprioriN</entry>

              <entry>Uses ‘new school’ techniques finding all patterns of up
              to N items, appearing together with a particular degree of
              support.</entry>
            </row>

            <row>
              <entry>EclatN</entry>

              <entry>Uses the ‘eclat’ technique to construct a result which is
              identical to AprioriN.</entry>
            </row>

            <row>
              <entry>Rules</entry>

              <entry>Uses patterns generated by AprioriN or EclatN to answer
              the question: “given a group of M items exists; what is the
              M+1th most likely to be”.</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para>The following performance statistics of these routines were
      observed using a 10 node cluster:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="4">
          <colspec align="left" colwidth="60pt" />

          <colspec align="left" colwidth="280pt" />

          <colspec align="left" colwidth="60pt" />

          <colspec align="left" colwidth="200pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>

              <entry align="left">Result</entry>

              <entry align="left">Expected Order</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Apriori1</entry>

              <entry>On 140M words</entry>

              <entry>47 secs</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Aprior1</entry>

              <entry>On 197M words</entry>

              <entry>91 secs</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 140M words, producing 2.6K pairs</entry>

              <entry>325 secs</entry>

              <entry>(N/k)^2.MLg(N) where k is the proportion of ‘buckets’ the
              average item is in. (Using terms in 5-10% of buckets)</entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 193M words (using .1-&gt;1% buckets) producing 4.4M
              pairs</entry>

              <entry>21 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori2</entry>

              <entry>On 19M words (10% sample), producing 4.1M pairs</entry>

              <entry>2 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>On 140M words (terms in 5-10% buckets)</entry>

              <entry>Exploded</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>1.9M words (1% sample of .1-1 buckets), (172K possible 3
              groups), 3.6B intermediate results, 22337 eventual
              results</entry>

              <entry>73 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>Apiori3</entry>

              <entry>On 1.9M words with new LOOKUP optimization</entry>

              <entry>42 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>EE3</entry>

              <entry>On 1.9M words (1% sample of .1-1 buckets), 22337 eventual
              results</entry>

              <entry>3 mins</entry>

              <entry></entry>
            </row>

            <row>
              <entry>EE10</entry>

              <entry>On 1.9M words</entry>

              <entry>Locks</entry>

              <entry></entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para role="nobrk">This example shows how to use the Apriori routine in
      conjunction with the Doc module. A simpler example can be found in the
      Association walk-through.</para>

      <para><programlisting role="nobrk"><?dbfo keep-together="always"?>IMPORT ML;
IMPORT ML.Docs AS Docs;

d11 := DATASET([{'One of the wonderful things about tiggers is tiggers are wonderful 
                 things'},
                {'It is a little scary the drivel that enters ones mind when given the 
                 task of entering random text'},
                {'I almost quoted Oscar Wilde; but I considered that I had gotten a little 
                 too silly already!'},
                {'I would hate to have quoted silly people!'},
                {'Oscar Wilde is often quoted'},
                {'In Hertford, Hereford and Hampshire Hurricanes hardly ever happen'},
                {'It is a far, far better thing that I do, than I have ever done'}],
                {string r});
d00 := DATASET([{'aa bb cc dd ee'},{'bb cc dd ee ff gg hh ii'},{'bb cc dd ee ff gg hh ii'},
                {'dd ee ff'},{'bb dd ee'}],{string r});

d := d11;
d1 := PROJECT(d,TRANSFORM(Docs.Types.Raw,SELF.Txt := LEFT.r));
d2 := Docs.Tokenize.Enumerate(d1);
d3 := Docs.Tokenize.Clean(d2);
d4 := Docs.Tokenize.Split(d3);                                      

lex := Docs.Tokenize.Lexicon(d4);

o1 := Docs.Tokenize.ToO(d4,lex);
o2 := Docs.Trans(O1).WordBag;

lex;
ForAssoc := PROJECT( o2, TRANSFORM(ML.Types.ItemElement,SELF.id := LEFT.id,
SELF.value := LEFT.word ));
ForAssoc;
ML.Associate(ForAssoc,2).Apriori1;
ML.Associate(ForAssoc,2).Apriori2;
ML.Associate(ForAssoc,2).Apriori3;
ML.Associate(ForAssoc,2).AprioriN(40);
</programlisting></para>

      <sect2>
        <title id="Associations_Example_OneClick">Example - Using ML.Associate
        on a One-Click<superscript>TM</superscript> Thor</title>

        <para>The HPCC One-Click<superscript>TM</superscript> Thor system is
        very quick and easy to setup and provides the perfect test environment
        for starting to use the HPCC ML Libraries. It is assumed that you have
        already installed the ECL IDE and the ML Module (which includes the
        example code needed). The data used is based on character information
        from Marvel comics and is taken from the Amazon Web Service Public
        Data area. It is available for immediate use by simply adding it to
        your One-Click<superscript>TM</superscript> Thor as a snapshot on
        launching the cluster.</para>

        <sect3>
          <title id="OneClick_Setup">Setting up the
          One-Click<superscript>TM</superscript> Thor</title>

          <para>You must have an Amazon AWS account to be able to setup a
          One-Click<superscript>TM</superscript> Thor cluster. To setup your
          One-Click<superscript>TM</superscript> Thor cluster:</para>

          <orderedlist>
            <listitem>
              <para>Go to the One-Click<superscript>TM</superscript> Thor
              <emphasis role="bold">Getting Started</emphasis> page on the
              HPCC Systems website: <ulink
              url="https://aws.hpccsystems.com/aws/getting_started/">https://aws.hpccsystems.com/aws/getting_started/</ulink>
              and following the <emphasis role="bold">Logging In</emphasis>
              instructions.</para>
            </listitem>

            <listitem>
              <para>Click <emphasis role="bold">Launch
              Cluster</emphasis>.</para>
            </listitem>

            <listitem>
              <para>Select the <emphasis role="bold">Region</emphasis>, which
              for the example data is <emphasis
              role="bold">Virginia</emphasis>.</para>
            </listitem>

            <listitem>
              <para>Enter the number of <emphasis role="bold">Thor
              Nodes</emphasis> you want (1 node is sufficient for this
              example).</para>
            </listitem>

            <listitem>
              <para>Enter the <emphasis role="bold">Snapshot ID</emphasis> for
              the example data as <emphasis
              role="bold">snap-7766d116</emphasis>.</para>
            </listitem>

            <listitem>
              <para>Press <emphasis role="bold">Launch
              Cluster</emphasis>.</para>
            </listitem>
          </orderedlist>

          <para><emphasis role="bold">Note:</emphasis> To add a snapshot of
          data from the Amazon Public Data Set , you need to know which
          <emphasis role="bold">Region</emphasis> it is linked with.</para>

          <para>The <emphasis role="bold">Launch Log</emphasis> shows the
          status of your One-Click<superscript>TM</superscript> Thor. When the
          configuration process is complete, the IP address of the ESP is
          shown in a link at the bottom of the page (xx.xx.xx.xxx.8010). Click
          on this link to access the <emphasis role="bold">ECL
          Watch</emphasis> page.</para>

          <para>Using <emphasis role="bold">ECL Watch</emphasis>, click on the
          <emphasis role="bold">DFU Files</emphasis>/<emphasis
          role="bold">Upload/download File</emphasis> menu option to view the
          contents of the <emphasis role="bold">Dropzone</emphasis>.</para>

          <para>The snapshot we attached to the
          One-Click<superscript>TM</superscript> Thor system is shown in the
          list and is ready to be used. Double-click on the folder to view the
          files which are included with the snapshot.</para>

          <para><graphic fileref="images/ML014.jpg" /></para>

          <para><emphasis role="bold">Note:</emphasis> You can use any data
          that is available on Amazon as well as any data that you have made
          and published as part of the Amazon Public Data Set. (If you want to
          know more about how to do this see, <ulink
          url="??http://aws.amazon.com/publicdatasets/">http://aws.amazon.com/publicdatasets/</ulink>.)
          You can also upload your own data from your own machine using the
          <emphasis role="bold">DFU Files/Upload/download File</emphasis> menu
          option in <emphasis role="bold">ECL Watch</emphasis> shown
          above.</para>
        </sect3>

        <sect3>
          <title id="One_Click_Running">Running the example using ECL
          IDE</title>

          <para>The ESP IP address changes every time you launch a new
          One-Click<superscript>TM</superscript> Thor cluster which means that
          you need to add a new ECL IDE configuration. On starting ECL IDE,
          click the <emphasis role="bold">Preferences</emphasis> button and
          enter the public ESP IP address in the <emphasis
          role="bold">Server</emphasis> field. Click <emphasis
          role="bold">OK</emphasis> to return to the login window, where you
          can select your new configuration and login.</para>

          <para><emphasis role="bold">Note:</emphasis> The public ESP IP
          address is available on the One-Click<superscript>TM</superscript>
          Thor <emphasis role="bold">Launch Log</emphasis>.</para>

          <para>The example ECL file is called <emphasis
          role="bold">Association_Marvel_Comics.ecl</emphasis> and is located
          by expanding the tree in the <emphasis role="bold">Repository
          Window</emphasis> as follows:</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">My Files</emphasis>/<emphasis
              role="bold">ML/Tests/Explanatory</emphasis></para>
            </listitem>
          </itemizedlist>

          <para>Open this file either by double-clicking on it or by using the
          <emphasis role="bold">Open in Builder Window</emphasis> right-click
          option.</para>

          <para>This file uses the Apriori3 and EclatN routines from the
          Associations Machine Learning Module to create character
          associations using the snapshot data (Marvel Universe Social Graph).
          There are some changes that need to be made to this ECL file:</para>

          <orderedlist>
            <listitem>
              <para>Change the <emphasis role="bold">Dropzone_IP</emphasis>
              address to use the private IP address. This information is
              available either from the One-Click<superscript>TM</superscript>
              Thor <emphasis role="bold">Launch Log</emphasis> or by using the
              <emphasis role="bold">System Servers</emphasis> menu item in
              <emphasis role="bold">ECL Watch</emphasis>.</para>
            </listitem>

            <listitem>
              <para>Change the <emphasis
              role="bold">Folder_Location</emphasis> to the location of the
              snapshot, (e.g.<emphasis role="bold"> <emphasis><emphasis
              role="bold">mnt::dropzone::snap-7766d116</emphasis></emphasis></emphasis><emphasis
              role="bold"><emphasis>)</emphasis><emphasis>.</emphasis></emphasis></para>
            </listitem>

            <listitem>
              <para>Change the <emphasis
              role="bold">Marvel_Inputfile</emphasis> name to the exact name
              of the file on the <emphasis role="bold">Dropzone</emphasis>.
              (In this case the file is called. <emphasis
              role="bold">labeled_edges.tsv</emphasis>)</para>
            </listitem>

            <listitem>
              <para>Select <emphasis role="bold">thor</emphasis> as the
              <emphasis role="bold">Target</emphasis>, and <emphasis
              role="bold">Submit</emphasis>.</para>
            </listitem>
          </orderedlist>

          <para>A green tick indicates that the job has completed. View the
          results by clicking on the completed workunit.<graphic
          fileref="images/ML015.jpg" /></para>
        </sect3>

        <sect3>
          <title id="OneClick_Results">Interpreting the results</title>

          <para>Using ECL Watch you can also view the progress, details and
          results of a workunit. It is a matter of preference whether you
          choose to use ECL IDE or ECL Watch. For the purposes of this
          example, results are shown using ECL Watch:</para>

          <orderedlist>
            <listitem>
              <para>Click on the <emphasis role="bold">ECL
              Workunits/Browse</emphasis> menu item and locate your workunit
              in the list.</para>
            </listitem>

            <listitem>
              <para>Click on the <emphasis role="bold">WUID</emphasis> link to
              display the <emphasis role="bold">Workunit Details</emphasis>
              page.</para>
            </listitem>

            <listitem>
              <para>Expand the <emphasis role="bold">Results</emphasis> link.
              Click on the rows link for any result to see the values<graphic
              fileref="images/ML016.jpg" /></para>
            </listitem>
          </orderedlist>

          <para></para>

          <para>The results produced reflect the Apriori3 and Eclat(N) example
          build process and are interpreted as follows:<informaltable>
              <tgroup cols="2">
                <colspec align="left" colwidth="120pt" />

                <colspec align="left" colwidth="450pt" />

                <thead>
                  <row>
                    <entry align="left">Result</entry>

                    <entry align="left">Description</entry>
                  </row>
                </thead>

                <tbody>
                  <row>
                    <entry>Transform File</entry>

                    <entry>Initial layout adding ID field to enumerate
                    rows.</entry>
                  </row>

                  <row>
                    <entry>Enumerate</entry>

                    <entry>Rows are numbered in sequential order.</entry>
                  </row>

                  <row>
                    <entry>Split Fields</entry>

                    <entry>Normalize and number the position of "word" in each
                    row.</entry>
                  </row>

                  <row>
                    <entry>Lexicon</entry>

                    <entry>Uses the Split Fields results to generate the total
                    for each "word" and in how many "docs".</entry>
                  </row>

                  <row>
                    <entry>Tokenize Fields</entry>

                    <entry>Adds new Lexicon fields to the Split Fields
                    result.</entry>
                  </row>

                  <row>
                    <entry>WordBag</entry>

                    <entry>Finds the maximum amount of words and docs and
                    counts the words in each doc.</entry>
                  </row>

                  <row>
                    <entry>Transform for Association</entry>

                    <entry>Transform into layout that is usable for creating
                    the associations.</entry>
                  </row>

                  <row>
                    <entry>Apriori3</entry>

                    <entry>ML.Associate([input dataset],[minimum
                    supports]).Apriori3; Uses ‘old school’ brute force and
                    speed approach to produce patterns of up to 3 items which
                    appear together with a particular degree of
                    support.</entry>
                  </row>

                  <row>
                    <entry>EclatN(3)</entry>

                    <entry>ML.Associate([input dataset],[minimum
                    supports]).EclatN([max item patterns]) Uses the ‘eclat’
                    technique for finding all patterns of up to N items,
                    appearing together with a particular degree of support.
                    Gives the exact same results as AprioriN.</entry>
                  </row>
                </tbody>
              </tgroup>
            </informaltable></para>
        </sect3>

        <sect3>
          <title id="Association_Example_Results">Apriori3 and EclatN(3)
          example results</title>

          <para>The Marvel Comics Social Graph data uses vertices that have
          numeric codes to represent names and comic books. The Find Captain
          America results contains both the code and code translations.</para>

          <para>The results shown include the top 10 results only.</para>

          <para><emphasis role="bold">Apriori 3 Association
          Results</emphasis></para>

          <para>On the results page, click on the rows link for the <emphasis
          role="bold">Apriori 3 Results</emphasis> to view the
          following:</para>

          <para></para>

          <para><graphic fileref="images/ML017.jpg" /></para>

          <para><emphasis role="bold">Values 1</emphasis>, <emphasis
          role="bold">2</emphasis> and <emphasis role="bold">3</emphasis>
          contain the codes given in the input file. <emphasis
          role="bold"></emphasis></para>

          <para><emphasis role="bold">Support</emphasis> shows the number of
          occurances where the association relationship was found.</para>

          <para></para>

          <para><emphasis role="bold">Apriori3 Find Captain America
          Results</emphasis></para>

          <para>On the results page, click on the rows link for the <emphasis
          role="bold">Captain America References Apriori3</emphasis> to view
          the following:</para>

          <para></para>

          <para><graphic fileref="images/ML018.jpg" /></para>

          <para><emphasis role="bold"></emphasis></para>

          <para><emphasis role="bold"></emphasis></para>

          <para><emphasis role="bold"></emphasis></para>

          <para><emphasis role="bold"></emphasis></para>

          <para><emphasis role="bold">EclatN(3) Association
          Results</emphasis></para>

          <para>On the results page, click on the rows link for the <emphasis
          role="bold">Eclat N Results</emphasis> to view the
          following:<graphic fileref="images/ML019.jpg" /></para>

          <para><emphasis role="bold">Pat</emphasis> contains the codes given
          in the input file. <emphasis role="bold"></emphasis></para>

          <para><emphasis role="bold">Support</emphasis> shows the number of
          occurances where the association relationship was found.</para>

          <para><emphasis role="bold">EclatN(3) Find Captain America
          Results</emphasis></para>

          <para>On the results page, click on the rows link for the <emphasis
          role="bold">Captain America References Eclat </emphasis>results to
          view the following:<graphic fileref="images/ML020.jpg" /></para>
        </sect3>
      </sect2>
    </sect1>

    <sect1 id="Classify">
      <title id="Classify_Module">Classify (ML.Classify)</title>

      <para>Use this module to tackle the problem, “can I predict this
      dependent variable based upon these independent ones?”. The following
      routines are provided:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="2">
          <colspec align="left" colwidth="130pt" />

          <colspec align="left" colwidth="490pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>BuildNaiveBayes</entry>

              <entry>Builds a Bayes model for one or more dependent
              variables.</entry>
            </row>

            <row>
              <entry>NaiveBayes</entry>

              <entry>Executes one or more Bayes models against an underlying
              dataset to compute dependent variables.</entry>
            </row>

            <row>
              <entry>TestNaiveBayes</entry>

              <entry>Generates a module containing four different measures of
              how well the classification models are doing. This calculation
              is based on the Bayes model and set of independent variables
              with outcome data supplied.</entry>
            </row>

            <row>
              <entry>BuildPerceptron</entry>

              <entry>Builds a perceptron for multiple dependent (Boolean)
              variables.</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para></para>
    </sect1>

    <sect1 id="Cluster">
      <title id="Cluster_Module">Cluster (ML.Cluster)</title>

      <para>This module is used to perform the clustering of a collection of
      records containing fields.</para>

      <para>The following routines are provided:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="70pt" />

            <colspec align="left" colwidth="430pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>DF</entry>

                <entry>A submodule which is used to perform various distance
                metrics upon two records. Currently, the following are
                provided: <itemizedlist>
                    <listitem>
                      <para>Euclidean</para>
                    </listitem>

                    <listitem>
                      <para>Euclidean Squared</para>
                    </listitem>

                    <listitem>
                      <para>Manhattan</para>
                    </listitem>

                    <listitem>
                      <para>Cosine</para>
                    </listitem>

                    <listitem>
                      <para>Tanimoto</para>
                    </listitem>
                  </itemizedlist>Quick variants exist for both Euclidean and
                Euclidean Squared which are much faster on sparse data
                PROVIDED you are willing to accept no distance, if there are
                no dimensions along which the vectors touch.</entry>
              </row>

              <row>
                <entry>Distances</entry>

                <entry>The engine to actually compute the distance matrix (as
                a matrix).</entry>
              </row>

              <row>
                <entry>Closest</entry>

                <entry>Takes a set of distances and returns the closest
                centroid for each row.</entry>
              </row>

              <row>
                <entry>KMeans</entry>

                <entry>Performs KMeans clustering for a specified number of
                iterations.</entry>
              </row>

              <row>
                <entry>AggloN</entry>

                <entry>Performs Agglomerative (Hierarchical) clustering. The
                results include cluster assignments, remaining distances
                between clusters and even the dentrogram.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <sect2 id="KMeans">
        <title>KMeans</title>

        <para>The KMeans module performs K-Means clustering on a static
        primary data set and a pre-determined set of centroids.</para>

        <para>This is an iterative two-step process. For each iteration, every
        data entity is assigned to the centroid that is closest to it, and
        then the centroid locations are re-assigned to the mean position of
        all the data entities assigned to them.</para>

        <para>The user passes in the two datasets and the maximum number of
        iterations to perform if convergence is not achieved.</para>

        <para>Optionally, the user may also define a convergence threshold
        (default=0.0) and the distance function to be used during calculations
        (default is Euclidean).</para>

        <para>The primary output from the KMeans module is “Result()”, which
        returns the centroid dataset with their new locations. “Convergence”
        will indicate the number of iterations performed, which will never be
        greater than the maximum number passed in to the function.</para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para></para>

        <para>There are also a number other useful return values that enable
        the user to analyze centroid movement. These are included in the
        following example.</para>

        <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;

lMatrix:={UNSIGNED id;REAL x;REAL y;};

// Simple two-dimensional set of numbers between 0-10
dDocumentMatrix:=DATASET([

  {1,2.4639,7.8579},
  {2,0.5573,9.4681},
  {3,4.6054,8.4723},
  {4,1.24,7.3835},
  {5,7.8253,4.8205},
  {6,3.0965,3.4085},
  {7,8.8631,1.4446},
  {8,5.8085,9.1887},
  {9,1.3813,0.515},
  {10,2.7123,9.2429},
  {11,6.786,4.9368},
  {12,9.0227,5.8075},
  {13,8.55,0.074},
  {14,1.7074,3.9685},
  {15,5.7943,3.4692},
  {16,8.3931,8.5849},
  {17,4.7333,5.3947},
  {18,1.069,3.2497},
  {19,9.3669,7.7855},
  {20,2.3341,8.5196}
],lMatrix);

// Arbitrary but non-symmetric centroid starting points
dCentroidMatrix:=DATASET([
  {1,1,1},
  {2,2,2},
  {3,3,3},
  {4,4,4}
],lMatrix);

// Convert the above matrices into the NumericField format
ML.ToField(dDocumentMatrix,dDocuments);
ML.ToField(dCentroidMatrix,dCentroids);

// Set up KMeans with a maximum of 30 iterations and .3 as a convergence threshold
KMeans:=ML.Cluster.KMeans(dDocuments,dCentroids,30,.3);

// The table that contains the results of each iteration
KMeans.Allresults;
// The number of iterations it took to converge
KMeans.Convergence;
// The results of iteration 3
KMeans.Result(3);
// The distance every centroid travelled across each axis from iterations 3 to 5
KMeans.Delta(3,5);
// The total distance the centroids travelled on each axis
KMeans.Delta(0);
// The straight-line distance travelled by each centroid from iterations 3 to 5
KMeans.DistanceDelta(3,5);
// The total straight-line distance each centroid travelled 
KMeans.DistanceDelta(0);
// The distance travelled by each centroid during the last iteration.
KMeans.DistanceDelta();
</programlisting></para>
      </sect2>

      <sect2 id="AggloN_function">
        <title id="Cluster_AggloN">AggloN</title>

        <para>The AggloN function performs Agglomerative clustering on a
        static document set. This is a bottom-up approach whereby close pairs
        are found and linked, and then treated as a single entity during the
        next iteration. Allowed to run fully, the result will be a single tree
        structure with multiple branches and sub-branches.</para>

        <para>The dataset and the maximum number of iterations are the two
        required parameters for this function. The user may also optionally
        specify the distance formula which defaults to Euclidean. The
        following is an example using the AggloN routine (using the same
        dDocuments constructed in the above example):</para>

        <para><programlisting><?dbfo keep-together="always"?>// Set up Agglomerative clustering with 4 iterations
AggloN:=ML.Cluster.AggloN(dDocuments,4);

// Table with nested sets of numbers delineating the tree after the specified
// number of iterations
AggloN.Dendrogram;
// Table containing the distances between each pair of clusters
AggloN.Distances;
// List of entities and the cluster they are assigned to (the cluster ID will always
// be the lowest ID value of the entities that comprise the cluster)
AggloN.Clusters;
</programlisting></para>
      </sect2>

      <sect2 id="Distances">
        <title id="Cluster_Distances">Distances</title>

        <para>This is the engine for calculating the distances between every
        entity in one dataset to every entity in a second dataset, which are
        the two required parameters. If the first dataset is also passed as
        the second one, then a self-join is performed. Otherwise, it is
        assumed that the IDs for the second dataset do not intersect with
        those from the first one. The default formula used for distance
        calculation is Euclidean, but that may be changed to any of the other
        distance functions available in the DF module.</para>

        <para>The code for the Distances function is fairly complex in an
        effort to facilitate both flexibility and efficiency. The user is able
        to calculate distances in three modes:</para>

        <para><informaltable>
            <?dbfo keep-together="always"?>

            <tgroup cols="2">
              <colspec align="left" colwidth="100pt" />

              <colspec align="left" colwidth="510pt" />

              <thead>
                <row>
                  <entry align="left">Routine</entry>

                  <entry align="left">Description</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>Dense</entry>

                  <entry>All calculations are performed, which for a self-join
                  is an N^2 problem.</entry>
                </row>

                <row>
                  <entry>Summary Join</entry>

                  <entry>Highly efficient but not as accurate, this process
                  only calculates distance between entities that share at
                  least one dimension.</entry>
                </row>

                <row>
                  <entry>Background</entry>

                  <entry>Fully accurate and highly efficient, assuming a
                  sparse matrix. This process takes advantage of the
                  assumption that only a small number of dimensions are shared
                  between any two pairs of entities.</entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>This is fully accurate and highly efficient,
        assuming a sparse matrix. This process takes advantage of the
        assumption that only a small number of dimensions are shared between
        any two pairs of entities.</para>
      </sect2>
    </sect1>

    <sect1 id="Correlations">
      <title id="Correlations_Module">Correlations (ML.Correlate)</title>

      <para>Use this module to calculate the degree of correlation between
      every pair of fields provided. The following routines are
      provided:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="100pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Simple</entry>

                <entry>Pearson and Spearman correlation co-efficients for
                every pair of fields.</entry>
              </row>

              <row>
                <entry>Kendal</entry>

                <entry>Kendal’s Tau for every pair of fields.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The correlation tables expose multicollinearity issues and enable
      you to look at predicted versus original to expose heteroscedasticity
      issues.</para>
    </sect1>

    <sect1 id="Discretize">
      <title id="Discretize_Module">Discretize (ML.Discretize)</title>

      <para>This module provides a suite of routines which allow a datastream
      with continuous real elements to be turned into a stream with discrete
      (integer) elements. The Discretize module currently supports three
      methods of discretization, ByRounding, ByBucketing and ByTiling. These
      method can be used by hand for reasons of simplicity and control.</para>

      <para>In addition, it is possible to turn them into an instruction
      stream for an 'engine' to execute. Using them in this way allows the
      discretization strategy to be in meta-data. Use the Do definition to
      construct the meta-data fragment, which will then perform the
      discretization. This enables the automatic generation of strategies and
      even iterates over the modeling process with different discretization
      strategies which can be programmatically generated. The following
      routines are provided:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="100pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>ByRounding</entry>

                <entry>Using scale and delta.</entry>
              </row>

              <row>
                <entry>ByBucketing</entry>

                <entry>Split the range evenly and distribute the values
                potentially unevenly.</entry>
              </row>

              <row>
                <entry>ByTiling</entry>

                <entry>Split the values evenly and have an uneven
                range.</entry>
              </row>

              <row>
                <entry>Do</entry>

                <entry>Constructs a meta-data fragment which will then perform
                the discretization.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The following is an example of the use of the Naive Bayes
      routine:</para>

      <para><programlisting><?dbfo keep-together="always"?>import ml;

value_record := RECORD
        unsigned rid;
        real height;
        real weight;
        real age;
        integer1 species;
        integer1 gender; // 0 = unknown, 1 = male, 2 = female
END;
d := dataset([{1,5*12+7,156*16,43,1,1},
              {2,5*12+7,128*16,31,1,2},
              {3,5*12+9,135*16,15,1,1},
              {4,5*12+7,145*16,14,1,1},
              {5,5*12-2,80*16,9,1,1},
              {6,4*12+8,72*16,8,1,1},
              {7,8,32,2.5,2,2},
              {8,6.5,28,2,2,2},
              {9,6.5,28,2,2,2},
              {10,6.5,21,2,2,1},
              {11,4,15,1,2,0},
              {12,3,10.5,1,2,0},
              {13,2.5,3,0.8,2,0},
              {14,1,1,0.4,2,0}
              ]
              ,value_record);
// Turn into regular NumericField file (with continuous variables)
ml.ToField(d,o);

// Hand-code the discretization of some of the variables
disc := ML.Discretize.ByBucketing(o(Number IN [2,3]),4)+ML.Discretize.ByTiling(o(Number IN
[1]),6)+ML.Discretize.ByRounding(o(Number=4));

// Create instructions to be executed
inst := 
ML.Discretize.i_ByBucketing([2,3],4)+ML.Discretize.i_ByTiling([1],6)+
ML.Discretize.i_ByRounding([4,5]);

// Execute the instructions
done := ML.Discretize.Do(o,inst);

//m1 := ML.Classify.BuildPerceptron(done(Number&lt;=3),done(Number&gt;=4));
//m1

m1 := ML.Classify.BuildNaiveBayes(done(Number&lt;=3),done(Number&gt;=4));
m1;

Test := ML.Classify.TestNaiveBayes(done(Number&lt;=3),done(Number&gt;=4),m1);
Test.Raw;
Test.CrossAssignments;
Test.PrecisionByClass;
Test.Headline;
</programlisting></para>

      <para>The following is an example of the use of the discretize
      routines:</para>

      <para><programlisting><?dbfo keep-together="always"?>import ml;
value_record := RECORD
                unsigned rid;
                real height;
                real weight;
                real age;
                integer1 species;
END;
d := dataset([{1,5*12+7,156*16,43,1},
              {2,5*12+7,128*16,31,1},
              {3,5*12+9,135*16,15,1},
              {4,5*12+7,145*16,14,1},
              {5,5*12-2,80*16,9,1},
              {6,4*12+8,72*16,8,1},
              {7,8,32,2.5,2},
              {8,6.5,28,2,2},
              {9,6.5,28,2,2},
              {10,6.5,21,2,2},
              {11,4,15,1,2},
              {12,3,10.5,1,2},
              {13,2.5,3,0.8,2},
              {14,1,1,0.4,2}             
              ]
              value_record);
// Turn into regular NumericField file (with continuous variables)
ml.macPivot(d,o);

// Hand-code the discretization of some of the variables
disc := ML.Discretize.ByBucketing(o(Number = 3),4)+ML.Discretize.ByTiling(o
(Number IN [1,2]),4)+ML.Discretize.ByRounding(o(Number=4));
disc;

// Create instructions to be executed
inst := ML.Discretize.i_ByBucketing([3],4)+ML.Discretize.i_ByTiling([1,2],4)
+ML.Discretize.i_ByRounding([4]);

// Execute the instructions
done := ML.Discretize.Do(o,inst);
done;
</programlisting></para>
    </sect1>

    <sect1 id="Distribution">
      <title id="Distribution_Module">Distribution (ML.Distribution)</title>

      <para>This module exists to provide code to generate distribution tables
      and ‘random’ data for a particular distribution.</para>

      <para>Each distribution is a ‘module’ that takes a collection of
      parameters and then implements a common interface. Other than the
      ‘natural’ mathematics of each distribution the implementation adds the
      notion of ranges (or NRanges). Essentially this means that the codomain
      (range) of the distribution is split into NRanges; this can be thought
      of as a degree of granularity. For discrete distributions this is
      naturally the number of results that can be produced. For continuous
      distributions you should think of the distribution curve as being
      approximated by NRanges straight lines. The maximum granularity
      currently supported in 1M ranges (after that the numeric pain of
      retaining precision is too nasty). The following routines are
      provided:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="150pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Uniform (Low, High, NRanges.)</entry>

                <entry>Specifies that any (continuous) value between Low and
                High is equally likely to occur.</entry>
              </row>

              <row>
                <entry>StudentT (degrees-of-freedon, NRanges)</entry>

                <entry>Specifies the degrees of freedom for a Student-T
                distribution. This module also exports InvDensity to provide
                the ‘t’ value that gives a particular density value (can be
                useful as the ‘tail’ of a t distribution is often
                interesting).</entry>
              </row>

              <row>
                <entry>Normal (Mean, Standard Deviation, NRanges)</entry>

                <entry>Implements a normal distribution (bell curve), which
                shows mean ‘mean’ and standard deviation as specified,
                approximate by NRanges straight lines.</entry>
              </row>

              <row>
                <entry>Exponential (Lamda, NRanges))</entry>

                <entry>Implements the exponential (sometimes called negative
                exponential) distribution.</entry>
              </row>

              <row>
                <entry>Binomial (p, NRanges)</entry>

                <entry>Gives the distribution showing the chances of getting
                ‘k’ successful events in Nranges-1 trials where the chances of
                success in one trial is ‘p’.</entry>
              </row>

              <row>
                <entry>NegBinomial (p, failures, NRanges)</entry>

                <entry>Gives the distribution showing the chances of getting
                ‘k’ successful events before ‘failures’ number of failures
                occurs (maximum total trials = nranges). The geometric
                distribution can be obtained by setting failures = 1.</entry>
              </row>

              <row>
                <entry>Poisson (Lamda, NRanges)</entry>

                <entry>For a poisson distribution the mean and variance are
                both provided by Lamda. It is a discrete distribution (will
                only produce integral values). Thus if NRanges is (say) 100
                then the probability function for Poisson will be computed for
                values of 0 to NRanges-1.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The interface to the distribution provides:</para>

      <para><programlisting><?dbfo keep-together="always"?>  EXPORT t_FieldReal Density(t_FieldReal RH)
// The probability density function at point RH

EXPORT t_FieldReal Cumulative(t_FieldReal RH)
// The cumulative probability function from – infinity[1] up to RH

  EXPORT DensityV() 
// A vector providing the probability density function at each range point – 
   this is approximately equal to the ‘distribution tables’ that might be published 
   in various books

EXPORT CumulativeV()
// A vector providing the cumulative probability density function at each range point 
   – again roughly equal to ‘cumulative distribution tables’ as published in the back of 
   statistics books

EXPORT Ntile(Pcnt
//provides the value from the underlying domain that corresponds to the given percentile. 
  Thus .Ntile(99) gives the value beneath which 99% of all should observations will fall.
</programlisting></para>

      <para>Most people do not really encounter StudentT as a distribution,
      rather they encounter the t-test. You can perform a t-test using the t
      distribution using the NTile capability. Thus the value for a
      single-tailed t-test with 3 degrees of freedom at the 99% confidence
      level can be obtained using:<programlisting><?dbfo keep-together="always"?>a := ML.Distribution.StudentT(3,10000);
a.NTile(99); // Single tail
a.NTile(99.5); // Double tail
</programlisting></para>

      <para><emphasis role="bold">Note:</emphasis> The Cauchy distribution can
      be obtained by setting v = 1.</para>

      <para>This module also exports:</para>

      <para><programlisting>GenData(NRecords,Distribution,FieldNumber)</programlisting>This
      allows N records to be generated each with a given field-number and with
      random values distributed according to the specified distribution. If a
      ‘single stream’ of random numbers is required then FieldNumber may be
      set to 1. It is provided to allow ‘random records’ with multiple fields
      to be produced.</para>

      <para>For example:<programlisting><?dbfo keep-together="always"?>IMPORT * FROM ML;
//a := ML.Distribution.Normal(4,5,10000);
a := ML.Distribution.Poisson(40,100);
//a := ML.Distribution.Uniform(0,100,10000);
a.Cumulative(5);
choosen(a.DensityV(),1000);
choosen(a.CumulativeV(),1000);
b := ML.Distribution.GenData(200000,a);
ave(b,value);
variance(b,value)
</programlisting></para>

      <para>The following performance timings were done generating 3 fields,
      two normal and one poisson:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="2">
          <colspec align="center" colwidth="130pt" />

          <colspec align="center" colwidth="130pt" />

          <thead>
            <row>
              <entry align="center">Records</entry>

              <entry align="center">Time (secs)</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>50K</entry>

              <entry>1.00</entry>
            </row>

            <row>
              <entry>500K</entry>

              <entry>2.29</entry>
            </row>

            <row>
              <entry>5M</entry>

              <entry>16.15</entry>
            </row>

            <row>
              <entry>25M</entry>

              <entry>80.2</entry>
            </row>

            <row>
              <entry>50M</entry>

              <entry>163</entry>
            </row>

            <row>
              <entry>100M</entry>

              <entry>316</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para></para>
    </sect1>

    <sect1 id="FieldAggregates_Module">
      <title id="Field_Aggregates_Module">FieldAggregates
      (ML.FieldAggregates)</title>

      <para>This module works on the field elements provided. It performs the
      tasks shown below for ALL the fields at once. The following routines are
      provided:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="130pt" />

            <colspec align="left" colwidth="470pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Simple stats</entry>

                <entry>Mean, Variance, Standard Deviation, Max, Min, Count,
                Sums, etc</entry>
              </row>

              <row>
                <entry>Medians</entry>

                <entry>Provides the median elements.</entry>
              </row>

              <row>
                <entry>Modes</entry>

                <entry>Provides the modal value(s) of each field.</entry>
              </row>

              <row>
                <entry>Cardinality</entry>

                <entry>Provides the cardinality of each field.</entry>
              </row>

              <row>
                <entry>Buckets/Bucket Ranges</entry>

                <entry>Divides the domain of each field evenly and then counts
                the number of elements falling into each range. Can be used to
                graphically plot the distribution of a field</entry>
              </row>

              <row>
                <entry>SimpleRanked</entry>

                <entry>Lists the ranking of the elements from the smallest to
                the largest.</entry>
              </row>

              <row>
                <entry>Ranked</entry>

                <entry>Adjusts the simple ranking to allow for repeated
                elements (each repeated element gets a rank which is the mean
                of the ranks provided to each element individually)</entry>
              </row>

              <row>
                <entry>NTiles/NTileRanges</entry>

                <entry>Think of this as giving the percentile rank of each
                element, except you get to pick if it is percentiles (N=100)
                or some other gradation</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>
    </sect1>

    <sect1 id="Regression">
      <title id="Regression_Module">Regression (ML.Regression)</title>

      <para>The Regression module has 2 implementations. There is a sparse
      matrix implementation, ML.Regression.Sparse, and a dense matrix
      implementation, ML.Regression.Dense. Both of these are an implementation
      of Ordinary Least Squares regression using either LU or Cholesky
      factorization.</para>

      <para>The regression attributes are:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="200pt" />

            <colspec align="left" colwidth="420pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Regression.Sparse.OLS_Cholesky</entry>

                <entry>Cholesky factorization of a sparse matrix for OLS
                regression.</entry>
              </row>

              <row>
                <entry>Regression.Sparse.OLS_LU</entry>

                <entry>LU factorization of a sparse matrix for OLS
                regression.</entry>
              </row>

              <row>
                <entry>Regression.Dense.OLS_Cholesky</entry>

                <entry>Cholesky of a dense matrix using the PB BLAS routines
                for OLS regression.</entry>
              </row>

              <row>
                <entry>Regression.Dense.OLS_LU</entry>

                <entry>LU factorization of a dense matrix using PB BLAS
                routines for an OLS regression.</entry>
              </row>

              <row>
                <entry>Regress_Poly_X</entry>

                <entry>Performs a polynomial regression for a single
                independent variable with a single dependent target.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The OLS routine can also return R^2 and Anova tables based upon
      the regression.</para>

      <para>Currently, Poly handles polynomials up to X^3 and includes
      logarithms. Along with the other ML functions Poly is designed to work
      upon huge datasets; however it can be quite useful even on tiny ones.
      The following dataset captures the time taken for a particular ML
      routine to execute against a particular number of records. It then
      produces a table showing the expected running time for any number of
      records that are entered:<programlisting>IMPORT ML;

R := RECORD
                INTEGER rid;
  INTEGER Recs;
                REAL Time;
                END;      
d := DATASET([{1,50000,1.00},{2,500000,2.29},         {3,5000000,16.15},{4,25000000,80.2},
                                                      {5,50000000,163},{6,100000000,316},
                                                      {7,10,0.83},{8,1500000,5.63}],R);

ML.ToField(d,flds);

P := ML.Regression.Poly(flds(number=1),flds(number=2),4);
P.Beta;
P.RSquared
</programlisting></para>

      <para>The R squared is a measure of goodness of fit.</para>
    </sect1>

    <sect1>
      <title id="Visualization_Module">Visualization Library (ML.VL)</title>

      <para>The Visualization Library is a set of routines designed to easily
      transition raw tabular data into an assortment of visually intuitive
      charts. The scope of the project is not to provide the charts
      themselves, but rather a standardized interface that can be used to
      connect to existing, publicly available charting APIs, such as Google
      Charts and D3. Using this library, the user may produce charts of many
      different types without consideration for the details that are specific
      to any one of those libraries.</para>

      <para>Using the library in its most rudimentary form can be as simple as
      this:</para>

      <programlisting>dFormatted:=VL.FormatData(dMyRawData,month);
VL.Chart(“MyLineChart”,dFormatted).Line;
VL.Chart(“ColumnChart02”,dFormatted).Column;
VL.Chart(“PieChart04”,dFormatted).Pie;
</programlisting>

      <para>The above calls will produce three charts (Line, Column and Pie,
      respectively), which can be viewed in the ECL IDE directly as individual
      charts, or as a published service in a “dashboard” type of format. More
      advanced capabilities are also available that enable the user to specify
      options either globally or on a chart-by-chart level, such as the
      chart’s dimensions, title, etc.</para>

      <sect2>
        <title id="VL_Setup">Setting up</title>

        <para>The assumption is being made that the ecl-ml repository has
        already been downloaded and placed in the user’s environment. The VL
        module is included in this suite of code. However, before the user can
        take advantage of the capabilities of the Visualization Library some
        changes need to be made to the environment so that the system knows
        how to create the charts. Specifically, the user needs to direct the
        compiler to a manifest file which contains the template information
        needed to translate the data into charts.</para>

        <para>The manifest file is located in the subfolder “XSLT” under the
        VL module. To link the compiler to it, simply add the following line
        as a parameter to your compiler calls:</para>

        <programlisting>-manifest “[the location of ecl-ml]\VL\XSLT\manifest.xml”</programlisting>

        <para>In the IDE, the user can specify compiler arguments by following
        these steps:</para>

        <orderedlist>
          <listitem>
            <para>On the IDE start-up screen, select <emphasis
            role="bold">Preferences</emphasis>.</para>
          </listitem>

          <listitem>
            <para>Select the <emphasis role="bold">Compiler</emphasis>
            tab.</para>
          </listitem>

          <listitem>
            <para>Enter the location of the manifest file (shown above) in the
            <emphasis role="bold">Arguments</emphasis> line.</para>
          </listitem>
        </orderedlist>

        <para><emphasis role="bold">Note: </emphasis>Compiler arguments are
        specific to the configuration, so if there are multiple
        configurations, ensure the proper one is selected. If the user wants
        to enable VL on all configurations, it will be necessary to place this
        option on each one individually.</para>
      </sect2>

      <sect2>
        <title id="VL_Parameters">Parameters</title>

        <para>A call to a chart in the VL framework needs three
        parameters:</para>

        <itemizedlist>
          <listitem>
            <para>A string containing the name to give the chart.</para>
          </listitem>

          <listitem>
            <para>A dataset containing the data to be presented, in the VL’s
            standard ChartData format.</para>
          </listitem>

          <listitem>
            <para>A Virtual Module containing the options to apply to the
            chart.</para>
          </listitem>
        </itemizedlist>

        <para>The first parameter is a simple string containing the name to
        give the chart. If more than one chart is called in a single job, it
        is important that this name is unique, or there will be a compiler
        error.</para>

        <para>The second parameter is the data. In order to simplify the
        movement of data through the various levels of the VL, all data passed
        into it is expected it to be in a uniform format: VL.Types.ChartData.
        The macro “FormatData” is provided which will translate data into this
        format, where the user passes in the name of the incoming dataset and
        the name of the column that contains the X-Axis values. In the above
        example, the dataset name is dMyRawData, and the X-Axis labels are
        located in the column “month”.</para>

        <para>The third parameter is a reference to the options that the user
        may adjust related to the position, size and other style-related
        attributes of the chart. This is a virtual module defined in
        VL.Styles. A default set of values is provided along with a few
        standard modifications that the user may desire (“Large”, “Small”,
        etc.). If the user does not specify the third parameter,
        VL.Styles.Default is assumed to be the options to apply.</para>
      </sect2>

      <sect2>
        <title id="VL_Specifying_API">Specifying an API Library</title>

        <para>The VL.Chart attribute is a generic entry point the enables the
        user to be unconcerned about the underlying structure of the
        Visualization Library. It takes on the responsibility of deciding for
        the user which API library to call to produce the chart in question.
        In the case where a chart may exist in more than API library, a choice
        has already been made as to which one is considered the “default” when
        requesting a pie chart, for example.</para>

        <para>There may be times, however, when a more advanced user want to
        have more control over which API library to call to produce a specific
        chart. In such a case, the user can call the interface to the API
        directly rather than through VL.Chart.</para>

        <para>For example:</para>

        <programlisting>VL.Chart(“MyLineChart”,dFormatted).Line;
VL.Google(“MyLineChart”,dFormatted).Line;
</programlisting>

        <para>These two lines produce the same results because Google is the
        default API for the Line chart type. But if the default were not
        Google, then the user could override that default by using the second
        line.</para>

        <para>There is an interface-layer attribute for each API library with
        which the VL library interacts. The parameter list for each will be
        the same which enables the user to easily switch API’s when
        desired.</para>
      </sect2>
    </sect1>
  </chapter>

  <chapter id="ML_with_documents">
    <title id="ML_DOCS">Using ML with documents (ML.Docs)</title>

    <para>The ML.Docs module provides a number of routines which can be used
    to pre-process text and make it more suitable for further processing. The
    following routines are provided:<informaltable>
        <tgroup cols="3">
          <colspec align="left" colwidth="80pt" />

          <colspec align="left" colwidth="90pt" />

          <colspec align="left" colwidth="430pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Sub-Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Tokenize</entry>

              <entry></entry>

              <entry>Routines which turn raw text into a clean processing
              format.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Enumerate</entry>

              <entry>Applies record numbers to the text for later
              tracking.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Clean</entry>

              <entry>Removes lots of nasty punctuation and some other things
              such as possessives.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Split</entry>

              <entry>Turns the document into a token (or word) stream.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Lexicon</entry>

              <entry>Constructs a dictionary (with various statistics) on the
              underlying documents.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>ToO/FromO</entry>

              <entry>Uses the lexicon to turn the word stream to and from an
              optimized token processing format.</entry>
            </row>

            <row>
              <entry>Trans</entry>

              <entry></entry>

              <entry>Performs various data transformations on an optimized
              document stream.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>WordBag</entry>

              <entry>Turns every document into a wordbag, by removing (and
              counting) multiple occurrences of a word within a
              document.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>WordsCounted</entry>

              <entry>Annotates every word in a document with the total number
              of times that word occurs in the document and distributes tfdi
              information for further (faster) processing.</entry>
            </row>

            <row>
              <entry>CoLocation</entry>

              <entry></entry>

              <entry>Functions related to the co-occurrence of NGrams and
              their constituents.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>AllNGrams</entry>

              <entry>A breakdown of all n-grams within a set of raw document
              text, where n is specified by the user.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>NGrams</entry>

              <entry>Aggregate information for every unique n-gram found
              within the document corpus.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Support</entry>

              <entry>The ratio of the number of documents which contain all of
              the n-grams in the parameter set compared to the corpus
              count.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Confidence</entry>

              <entry>The ratio of the number of documents which contain all of
              the n-grams in both parameter sets, compared to the number of
              documents which only contain the n-grams in the first
              set.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Lift</entry>

              <entry>The ratio of observed support of two sets of n-grams
              compared to the support if the n-grams were independent.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>Conviction</entry>

              <entry>The ratio of how often one set of n-grams exists without
              a second set given that the two are independent.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>SubGrams</entry>

              <entry>Compares the product of the document frequencies of all
              constituent unigrams to the n-gram they comprise.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>SplitCompare</entry>

              <entry>Compares the document frequency of every n-gram to the
              individual frequencies of a member unigram and the remaining
              n-1-gram.</entry>
            </row>

            <row>
              <entry></entry>

              <entry>ShowPhrase</entry>

              <entry>If lexical substitution was using in constructing the
              AllNGrams dataset, this function will re-constitute the text
              associated with their corresponding numbers within an
              n-gram.</entry>
            </row>

            <row>
              <entry>PorterStem</entry>

              <entry></entry>

              <entry>Standard Porter stemmer in ECL.</entry>
            </row>

            <row>
              <entry>PorterStemC</entry>

              <entry></entry>

              <entry>Standard Porter stemmer in C (faster).</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable></para>

    <sect1 id="Doc_module_usage">
      <title id="DOCS_Usage">Typical usage of the Docs module</title>

      <para>The following code demonstrates how the docs module may be used
      and show the result at each stage:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
IMPORT ML.Docs AS Docs;

d := DATASET([{'One of the wonderful things about tiggers is tiggers are wonderful things'},
                                  {'It is a little scary the drivel that enters ones mind 
                                    when given the task of entering random text'},
                                  {'I almost quoted Oscar Wilde; but I considered that I 
                                    had gotten a little too silly already!'},
                                  {'In Hertford, Hereford and Hampshire Hurricanes hardly 
                                    ever happen'},
                                  {'It is a far, far better thing that I do, than I have 
                                    ever done'}],{string r});

d1 := PROJECT(d,TRANSFORM(Docs.Types.Raw,SELF.Txt := LEFT.r));

d1;

d2 := Docs.Tokenize.Enumerate(d1);

d2;

d3 := Docs.Tokenize.Clean(d2);

d3;

d4 := Docs.Tokenize.Split(d3); 

d4;

lex := Docs.Tokenize.Lexicon(d4);

lex;

o1 := Docs.Tokenize.ToO(d4,lex);
o1;

Docs.Trans(o1).WordBag;
Docs.Trans(o1).WordsCounted;

o2 := Docs.Tokenize.FromO(o1,lex);
o2;
</programlisting></para>
    </sect1>

    <sect1 id="Performance_statistics">
      <title id="Docs_Performance">Performance Statistics</title>

      <para>The following performance statistics of these routines were
      observed using a 10 node cluster:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="4">
          <colspec align="left" colwidth="120pt" />

          <colspec align="left" colwidth="280pt" />

          <colspec align="left" colwidth="80pt" />

          <colspec align="left" colwidth="120pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>

              <entry align="left">Result</entry>

              <entry align="left">Expected Order</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Clean and Split</entry>

              <entry>22m documents producing 1.5B words.</entry>

              <entry>60 minutes</entry>

              <entry>Linear</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>1.5B words producing 6.4M entries.</entry>

              <entry>40 minutes</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating 'working' entries from 1.5B words and the full
              6.4m entry lexicon.</entry>

              <entry>46 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating ‘working’ entries from 1.5B words and the
              ‘keyword’ lexicon, produces 140M words.</entry>

              <entry>37 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating ‘working’ entries from 1.5B words and the
              ‘keyword MkII’ lexicon, produces 240M words.</entry>

              <entry>40 minutes</entry>

              <entry>NLgN</entry>
            </row>

            <row>
              <entry>Lexicon</entry>

              <entry>Creating ‘working’ entries from 1.5B words and the
              ‘keyword MkII’ lexicon, produces 240M words, using the ‘small
              lexicon’ feature.</entry>

              <entry>7 minutes</entry>

              <entry>Linear</entry>
            </row>

            <row>
              <entry>Wordbag</entry>

              <entry>Create WordBags from 140M words.</entry>

              <entry>114 seconds</entry>

              <entry>NlgN</entry>
            </row>

            <row>
              <entry>Wordbag</entry>

              <entry>Create WordBags from 240M-&gt;193M words.</entry>

              <entry>297 seconds</entry>

              <entry>NlgN</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
    </sect1>
  </chapter>

  <chapter id="ML_implementation">
    <title id="Useful_Routines">Useful routines for ML implementation</title>

    <para>The following modules are provided to help with ML
    implementation:</para>

    <itemizedlist>
      <listitem>
        <para>The Utility module</para>
      </listitem>

      <listitem>
        <para>The Matrix Library (Mat)</para>
      </listitem>

      <listitem>
        <para>The Dense Matrix Library (DMat)</para>
      </listitem>

      <listitem>
        <para>Parallel Block BLAS for ECL</para>
      </listitem>
    </itemizedlist>

    <sect1 id="Utility">
      <title id="Routines_Utility">Utility</title>

      <para>The Utility module provides the following routines:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="250pt" />

            <colspec align="left" colwidth="350pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>ML.ToFieldElement/FromFieldElement</entry>

                <entry>Translates in and out of the core field-element
                datamodel.</entry>
              </row>

              <row>
                <entry>ML.Types.ToMatrix/FromMatrix</entry>

                <entry>Translates from field elements to and from
                matrices.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>
    </sect1>

    <sect1 id="Matrix_Library">
      <title id="Routines_Matrix">The Matrix Library (Mat)</title>

      <para>This library is in addition to and subservient to the ML library
      modules. The following routines are provided:</para>

      <para><informaltable>
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec align="left" colwidth="110pt" />

            <colspec align="left" colwidth="490pt" />

            <thead>
              <row>
                <entry align="left">Routine</entry>

                <entry align="left">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Add</entry>

                <entry>Add two matrices.</entry>
              </row>

              <row>
                <entry>Decomp</entry>

                <entry>Matrix Decomposition: LU, Choleski and QR.</entry>
              </row>

              <row>
                <entry>Det</entry>

                <entry>Determinant of a matrix.</entry>
              </row>

              <row>
                <entry>Each</entry>

                <entry>Some ‘element by element’ processing steps.</entry>
              </row>

              <row>
                <entry>Eig</entry>

                <entry>Decompose matrix into eigenvalue and eigenvector
                component matrices</entry>
              </row>

              <row>
                <entry>Eq</entry>

                <entry>Are two matrices equal.</entry>
              </row>

              <row>
                <entry>Has</entry>

                <entry>Various matrix properties.</entry>
              </row>

              <row>
                <entry>Identity</entry>

                <entry>Construct an Identity Matrix.</entry>
              </row>

              <row>
                <entry>InsertColumn</entry>

                <entry>Add a column to a matrix.</entry>
              </row>

              <row>
                <entry>Inv</entry>

                <entry>Invert a matrix.</entry>
              </row>

              <row>
                <entry>Is</entry>

                <entry>Boolean tests for certain matrix types (Identity, Zero,
                Diagonal, Triangular etc).</entry>
              </row>

              <row>
                <entry>Lanczos</entry>

                <entry>Lanczos decomposition</entry>
              </row>

              <row>
                <entry>MU</entry>

                <entry>Matrix Universe. A number of routines to allow multiple
                matrices to exist within the same ‘file’ (or dataflow). Useful
                for iterating around loops etc.</entry>
              </row>

              <row>
                <entry>Mul</entry>

                <entry>Multiply two matrices.</entry>
              </row>

              <row>
                <entry>Pca</entry>

                <entry>Principal Component Analysis</entry>
              </row>

              <row>
                <entry>Pow</entry>

                <entry>Multiplies a matrix by itself N-1 * (and demo’s
                MU).</entry>
              </row>

              <row>
                <entry>Repmat</entry>

                <entry>Construct a matrix consisting of M-by-N tiling copies
                of the original matrix</entry>
              </row>

              <row>
                <entry>RoundDelta</entry>

                <entry>Round all the elements of a matrix if they are within
                delta of an integer.</entry>
              </row>

              <row>
                <entry>Scale</entry>

                <entry>Multiply a matrix by a constant.</entry>
              </row>

              <row>
                <entry>Sub</entry>

                <entry>Subtract one matrix from another.</entry>
              </row>

              <row>
                <entry>Substitute</entry>

                <entry>Construct a matrix which is all the elements of the
                right + any elements from the left which are not in the
                right.</entry>
              </row>

              <row>
                <entry>Svd</entry>

                <entry>Singular Value Decomposition</entry>
              </row>

              <row>
                <entry>Thin</entry>

                <entry>Make sure a matrix is fully sparse.</entry>
              </row>

              <row>
                <entry>Trans</entry>

                <entry>Construct the transpose of a matrix.</entry>
              </row>

              <row>
                <entry>Vec</entry>

                <entry>A vector library, to create vectors from matrices and
                assign vectors into matrices.</entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <para>The following examples demonstrate some of these routines:</para>

      <para><programlisting><?dbfo keep-together="always"?>IMPORT ML;
IMPORT ML.Mat AS Mat;
d := dataset([{1,1,1.0},{1,2,2.0},{2,1,3.0},{2,2,4.0}],Mat.Types.Element);

Mat.Sub( Mat.Scale(d,10.0), d );
Mat.Mul(d,d);
Mat.Trans(d);
</programlisting></para>
    </sect1>

    <sect1 id="Routines_DenseMatrix">
      <title>The Dense Matrix Library (DMat)</title>

      <para>This module provides the following routines:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="2">
          <colspec align="left" colwidth="110pt" />

          <colspec align="left" colwidth="490pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>Add</entry>

              <entry>Sums two matrices.</entry>
            </row>

            <row>
              <entry>Converted</entry>

              <entry>Module for several different conversions.</entry>
            </row>

            <row>
              <entry>Decomp</entry>

              <entry>Decompose a matrix.</entry>
            </row>

            <row>
              <entry>Identity</entry>

              <entry>Create an identity matrix.</entry>
            </row>

            <row>
              <entry>Mul</entry>

              <entry>Multiply two matrices.</entry>
            </row>

            <row>
              <entry>Scale</entry>

              <entry>Multiplies a matrix by a scalar.</entry>
            </row>

            <row>
              <entry>Sub</entry>

              <entry>Subtracts two matrices.</entry>
            </row>

            <row>
              <entry>Trans</entry>

              <entry>Matrix transpose</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para>These routines are intended to be similar to the Mat
      module.</para>
    </sect1>

    <sect1 id="Routines_BLAS">
      <title>Parallel Block BLAS for ECL</title>

      <para>The ML Libraries provide an implementation of selected BLAS
      operations for use in a Parallel Block matrix application. The BLAS
      operations work on double precision floating point numbers. The
      functions provided are: matrix and matrix multiply and sum; scalar and
      matrix multiply and sum; scale vector; symmetric rank update; triangular
      matrix solver; Cholesky factorization; and LU factorization.</para>

      <para>To use the Parallel Block BLAS routines, the user must decide on
      the partitioning for the matrices. The PBblas.Matrix_Map attribute is
      used to communicate the matrix and partition dimensions to the BLAS
      attributes. It is the responsibility of the user to employ compatible
      partitioning schemes.</para>

      <para>The PBblas module provides the following routines:</para>

      <informaltable>
        <?dbfo keep-together="always"?>

        <tgroup cols="2">
          <colspec align="left" colwidth="110pt" />

          <colspec align="left" colwidth="490pt" />

          <thead>
            <row>
              <entry align="left">Routine</entry>

              <entry align="left">Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>MakeR8Set</entry>

              <entry>Packs a dataset of cells for a partition into a set of
              double precision numbers.</entry>
            </row>

            <row>
              <entry>Matrix_Map</entry>

              <entry>Specifies the dimensions of a matrix and the dimensions
              of the partitions.</entry>
            </row>

            <row>
              <entry>PB_daxpy</entry>

              <entry>Multiplies vector X by a scalar and sums vector
              Y.</entry>
            </row>

            <row>
              <entry>PB_dbvmm</entry>

              <entry>Efficient matrix multiply when the result is a single
              partition. Multiplies the product of matrices A and B with a
              scalar alpha, and then sums with matrix C multiplied by a scalar
              beta.</entry>
            </row>

            <row>
              <entry>PB_dbvrk</entry>

              <entry>Matrix update with a block vector.</entry>
            </row>

            <row>
              <entry>PB_dgemm</entry>

              <entry>General purpose matrix multiply. Multiplies a the product
              of matrices A and B with a scalar alpha, and then sums with
              matrix C multiplied by a scalar beta.</entry>
            </row>

            <row>
              <entry>PB_dgetrf</entry>

              <entry>LU Factorization.</entry>
            </row>

            <row>
              <entry>PB_dpotrf</entry>

              <entry>Cholesky factorization.</entry>
            </row>

            <row>
              <entry>PB_dscal</entry>

              <entry>Scalar multiply.</entry>
            </row>

            <row>
              <entry>PB_dtran</entry>

              <entry>Matrix transpose.</entry>
            </row>

            <row>
              <entry>PB_dtrsm</entry>

              <entry>Solves either Ax = B or xA = B.</entry>
            </row>

            <row>
              <entry>PB_Extract_Tri</entry>

              <entry>Extracts the upper or lower triangular. Useful for
              extracting the L and U matrices from the composite matrix
              created by the LU factorization.</entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <sect2>
        <title>Installation notes</title>

        <para>The ML Libraries can use the BLAS libraries to speed up certain
        matrix operations. You may install BLAS (Basic Linear Algebra System)
        or ATLAS, or some other package containing a BLAS. The developer
        version should be installed.</para>

        <para>The Attributes that employ BLAS require the cblas.h header file
        and the cblas library. These files must be located in directories
        specified in default search paths for headers and libraries. In some
        systems, you may need to use a symbolic link. For example, in CentOS
        the header file is installed in a usual place but the library is
        installed in /usr/lib64 which is not where the linker looks by
        default.</para>
      </sect2>
    </sect1>
  </chapter>
</book>
